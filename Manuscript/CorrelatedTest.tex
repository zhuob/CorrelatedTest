\documentclass[12pt, a4paper]{article}
\usepackage{graphicx}
\usepackage{amsmath,bm}
\usepackage[square,numbers]{natbib}
%\usepackage[square, numbers, authoryear]{natbib}
\usepackage[utf8]{inputenc}    
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{multirow}
\title{Estimating test statistics correlation from sample correlation}
\date{\today} % Today's date or a custom date
\usepackage{epsfig}
\usepackage{subcaption}




\newtheorem{theorem}{Theorem}       % theorem environment
\newtheorem{definition}{Definition}     % definition environment
\newtheorem{proof}{Proof}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{corollary}

\newcommand{\cov}{\text{Cov}}
\newcommand{\cor}{\text{Corr}}
\newcommand{\var}{\text{Var}}
\newcommand{\samplecor}{sample correlation}
\newcommand{\popucor}{population correlation}

\begin{document}
	\maketitle
	\section{Introduction}
%	\textbf{What's the consequence if the correlation between statistics cannot be represented sample correlation?}
%	\begin{enumerate}
%		\item Methods relating FDR control in terms of type I error seems to be OK?? Because under the null, test statistics correlation are (almost) the same as sample correlation.
%		\item What about power in terms of FDR control?
%		\item Competitive gene set test would definitely be affected, in terms of both type I error and power.  
%		\item it seems, according to Efron's 2007 paper, that conditional FDR will also be affected. 
%	\end{enumerate}
%	
%	\textbf{What problem do we address in this paper?}\\
%
%	
%	\textbf{Introduction}\\

	In gene expression experiments, inter-gene correlations are commonly observed in expression data \citep{efron2012large, qiu2005effects,barry2008statistical, efron2007correlation,storey2003positive, huang2013gene, wu2012camera, gatti2010heading}.
	 The key task of expression analysis is to detect differentially expressed (DE) genes. One common feature of such DE detection is that a
	  summary statistic is calculated for each gene to measure the magnitude of DE. The test statistics are often of familiar form, for example, 
	  they may come from two-sample comparison or experimental design based regression models. However, those test statistics are likely to be 
	  correlated, since their 
	  corresponding expression levels are correlated. This paper concerns the relation between test statistics correlations and the 
	  corresponding expression level correlations.
	
	\textbf{Why would people care about correlation between genes?}\\
	 The stochastic dependence of test statistics has brought methodological issues, in terms of accessing both individual genes and gene sets. 
	 The interest in examining individual genes is to find DE genes among tens of thousands of candidates. Multiple hypothesis testing 
	 procedures, such as \textit{false discovery rate} (FDR) \citep{benjamini1995controlling} and \textit{$q$-value} \citep{storey2003positive}, 
	 are therefore needed. In many cases, such techniques work only when test statistics are independent \citep{benjamini1995controlling} or 
	 have positive regression dependency \citep{benjamini2001control}. The goal of evaluating gene sets is to find molecular pathways or gene 
	 networks that are related to the experimental condition or factors of interest. Testing a gene set is usually done by pooling the test 
	 statistics of its member genes, and may or may not involve genes not in the test set \citep{goeman2007analyzing}. In all situations, the 
	 correlation between test statistics is a nuisance aspect, which, if not addressed appropriately, will undermine the applicability of the 
	 corresponding approaches (REF). For
	 example, \citet{efron2007correlation} showed in a simulation study that for a nominal FDR of 0.1, the
	 actual FDR can easily vary by a factor of 10 when correlation between test statistics exists. 
	 
	 \textbf{What are existing ways of dealing with inter-gene correlations?}\\
	 A number of attempts have been made to deal with issues of inter-gene correlation when testing either individual genes or gene sets. One 
	 approach is to derive certain summary statistic from correlation among test statistics and then use it in the hypothesis testing procedure. 
	 (Do I need more examples here) For testing individual genes, \citet{efron2007correlation} calculates the \textit{false discovery 
	 proportion} (FDP) conditioning on some dispersion variate which is estimated from correlation among transformed test statistics.  For 
	 testing gene sets, \citet{wu2012camera} estimate a \textit{variance inflation factor} (VIF) associated with inter-gene correlation and 
	 incorporate it into their parametric/rank-based testing procedures. The same VIF is also used by \citet{yaari2013quantitative} to account 
	 for correlation in their distribution-based gene set testing procedure. Another approach is to permute the labels of biological samples. 
	 Sample permutation generates the null distribution of test statistic for each gene.
	 This type of permutation preserves underlying correlation structure between genes, and thus protect the test against such correlations 
	 (REF, FDR related and gene set test related). However, sample permutation method has an extra assumption, which states that the test 
	 statistics always follow the distribution they have under complete null  that no gene is DE \cite{efron2012large1}. In other words, this 
	 assumption expects that the distribution of test statistics under the null is not affected by the presence of non-null cases. The 
	 \textit{gene set enrichment analysis} (GSEA) procedure \citep{subramanian2005gene} falls into this category.
	 
	\textbf{Key question: Are  expression level correlations the same as test statistics correlation?}\\
	The first approach requires that the correlations between test statistics are known or at least can be estimated from the data. Without 
	replicating the experiment, however, there's no way to obtain the correlation structure of test statistics because only a single test 
	statistic is available for each gene. In the case of one-sided test (e.g., two sample $t$-test), one possible choice is to use sample 
	correlations (after gene treatment effects nullified) to represent correlations among test statistics, as is done by 
	\citet{barry2008statistical, efron2007correlation, wu2012camera}. 
	In all of the three works, it is shown by simulation only the
	equivalence (in terms of either distribution or numerical summarization) of sample correlation coefficient and test statistics correlation coefficient. 
	\citet{efron2007correlation} estimates the distribution of $z$-value (transformed from corresponding two sample $t$-test statistics) 
	correlation by sample correlation. \citet{barry2008statistical} show by Monte Carlo simulation of gene expression data that a nearly linear 
	relationship holds between test statistic correlation and sample correlation for several types of test statistic.
	%The validity of such representation is then demonstrated by small simulations.
	% \citet{efron2007correlation} used $z$-values converted from corresponding two sample $t$-test statistics by 
%	\begin{equation}\label{t2z}
%		z  = \Phi^{-1}(G_0(t))
%	\end{equation}
%	where $\Phi$ is the cumulative distribution function (CDF) for $N(0, 1)$ and $G_0$ is a putative
%	null CDF for $t$-values. The correlation of $z$-values were approximated by residual sample
%	correlation, since it was demonstrated via simulation that the distribution of residual sample
%	correlation applies to that of $z$-value correlation. \citet{barry2008statistical} showed by Monte
%	Carlo simulation of gene expression data that a nearly linear relationship holds between test
%	statistic correlation and residual sample correlation under several standard experimental design.
%	The sample correlation was then used to account for test statistics correlation in their bootstrap
%	method of enrichment test. \citet{wu2012camera} assumed that genewise $t$-test statistics correlation
%	is the same as residual sample correlation, and calculated the mean of all pairwise sample
%	correlation to estimate the VIF, a vital factor in adjusting for inter-gene correlation in their
%	enrichment test procedures. 
	 It has, to the best of our knowledge, not yet been fully explored in
	the context of two group comparison.
	
	
		\textbf{What did we find}\\
		In this work, we investigated the effect of testing procedures on inter-gene correlation structure
		regarding two group comparison. Theoretically, we proved that for two sample $z$-test, there is a
		perfect positive correlation between sample correlation coefficient $r_{\text{sample}}$ and test
		statistics correlation $r_{\text{statistic}}$. For two sample $t$-test, the equivalence does not
		hold in general for $r_{\text{statistic}}$ and $r_{\text{sample}}$, unless all the test are true
		null (no DE).  We demonstrated by simulation that under the null, such equivalence also holds for
		two group comparison of Poisson regression. 
		
		
	
	\textbf{Relevant but different work}\\
	A relevant research was done by \citet{qiu2005effects}, in which they studied the effect of different
	normalization procedures on the inter-gene correlation structure for microarray data. They randomly
	assigned 330 arrays into 15 pairs, each containing 22 arrays within each array 12558 genes. Then 15
	$t$-statistics were calculated for each gene to mimic 15 two-sample comparisons under null
	hypothesis of no DE. They compared the histogram of $t$-statistics correlation for different
	normalization algorithms, and concluded that the normalization procedures are unable to completely
	remove the correlation between the test statistics. % In this work, our interest is in evaluating
	%the effect of several testing procedures on gene expression correlation. 
	

	
	
	
	
	
	\section{General setup}
	
	\subsection{define what do we mean by correlation}
	\textit{Correlation} is a statistical quantity used to assess a possible linear relationship between two random variables or two sets of 
	data sets. The degree of correlation is measured by \textit{correlation coefficient}, a scaler taking values on the interval $[-1, 1]$. 
	Correlation coefficient of $+1$ ($-1$) indicates perfect positive (negative dependence), while correlation coefficient of 0 implies no linear 
	relationship between two random variables. Larger correlation coefficient (in absolute value) corresponds to stronger linear correlation. 
	There are many ways to look at the correlation coefficient , many of which are special cases of Pearson's correlation coefficient 
	\citep{lee1988thirteen}. For example, the \textit{Kendall tau rank correlation coefficient} is computed as Pearson's correlation coefficient 
	between the ranked variables.
	
	Let $(X, Y)$ be a random vector, and $(x_j, y_j)$ its $j$th observation. The most familiar measure of dependence between two quantities is 
	the \textit{Pearson's correlation coefficient}.  Following the notation of \citet{lee1988thirteen}, We will restrict our interest to two 
	types of Pearson's correlation coefficient: 1) standardized covariance, which we refer to as \textit{\popucor}
	\begin{equation}\label{eq:popucor}
		\rho =\dfrac{\cov(X, Y)}{\sqrt{\var(X)\var(Y)}} = \dfrac{E[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X\sigma_Y}, 
	\end{equation} 	where $\mu_X$ and $\mu_Y$ are the expected values and $\sigma_X<\infty$ and $\sigma_Y<\infty$ are the population standard errors, and 2) a function of raw scores and means, which we refer to as \textit{\samplecor}
	\begin{equation}\label{eq:samplecor}
	r  =  \dfrac{\sum_j (x_j -\bar{x})(y_j - \bar{y})}{\sqrt{\sum_{j}(x_j - \bar{x})^2\sum_i(y_j - \bar{y})^2}}, 
	\end{equation}
	where $(\bar{x}, \bar{y})$ is the vector of arithmetic mean of the observations.
	Throughout this paper, we will discuss the correlation between $X$ and $Y$ under bivariate settings. 


	 We assume that the population mean of $(X_j, Y_j)$ may differ across samples, but that the population covariance structure remains the 
	 same, that is,  
		\begin{equation}\label{eq:meanstruct}
		\begin{aligned}
		E  \left(\begin{array}{c}
		X_j\\
		Y_j\\	
		\end{array} \right) 
		 = &	\left(\begin{array}{c}
		\mu_{X,j}\\
		\mu_{Y,j}\\
		\end{array} \right)\stackrel{\text{def}}{=} \bm \mu_j,\\
		\cov\left(\begin{array}{c}
		X_j\\
		Y_j\\	
		\end{array} \right)	
		= &\left(
		\begin{array}{cc}
		\sigma_X^2 &\rho \sigma_X\sigma_Y \\
		\rho \sigma_X \sigma_Y & 	\sigma_Y^2 \\
		\end{array} 
		\right)
		\stackrel{\text{def}}{=} \bm \Sigma,  \text{~~ for $j = 1, \ldots, n$}
			\end{aligned}
		\end{equation}
	where $\rho$ is the population correlation defined by equation (\ref{eq:popucor}).
	
	In addition, we assume independence across samples (Note that independence implies 0 correlation, but not vise versa), 
	\begin{equation}\label{eq:indepsamples}
	\cov(X_{j_1}, X_{j_2}) = \cov(Y_{j_1}, Y_{j_2}) = 0 \text{~~~for $j_1\neq j_2$}
	\end{equation}
	 In the context of gene expression study, the goal is to detect differential expression (DE)---whether the expression level of a gene is 
	 significantly correlated with treatment or experimental variables. Let $\bm a:=(a_1, \ldots, a_n)^T$ be a vector for a contrast of 
	 interest, then DE detection for gene $X$ can be statistically formulated as 
	 \begin{equation}\label{eq:hypotheses}
	 H_{0}:  \bm a^T\bm \mu_X = d_X \textit{     Verses   }  H_{1}: \bm a^T\bm \mu_X \neq d_X,
	 \end{equation}
	 where $\bm X = (X_1, \ldots, X_n)^T$ and $\bm \mu_{X} = (\mu_{X, 1}, \ldots, \mu_{X, n})^T$. 
	This hypothesis testing procedure usually results in a ``$t$-test similar" test statistic, in which the numerator is a linear combination of 
	$\bm X$ and the denominator is its standard error. Without a loss of generality, we express the test statistics as follows
	\begin{equation}\label{eq:teststat}
	T_X = \dfrac{\bm a^T\bm X}{S_X},  ~~~ T_Y = \dfrac{\bm a^T \bm Y}{S_Y}
	\end{equation}  
	$S_X$ and $S_Y$ are the standard error for $\bm a^T\bm X$ and $\bm a^T\bm Y$ respectively.
	
	Our main goal is to explore the relationship between population correlation (equation (\ref{eq:popucor})) for the test statistics 
	\begin{equation}
	\rho_T(n) = \cor({T_X, T_Y}),
	\end{equation}  
	and that for their corresponding expression level 
	\begin{equation}
	\rho = \cor(X, Y). 
	\end{equation}
	We will examine ???HOW MANY??? different test statistics having the form of equation (\ref{eq:teststat}).%, for a given sample size of $n_1$ for treatment group and $n_2$ for control group.
		

	
	\section{Results}\label{section:tcorresults}
	
	In this section we present the exact expression of statistics correlation coefficient for two
	sample $t$-test. In the first part, we conclude theoretically that test statistics correlation and
	sample correlation are perfect positive dependent for two sample $z$-test, but that is not always
	true for two sample $t$-test. In the second part, we simulate four different cases where test
	statistics correlation $r_{\text{statistics}}$ may be very different from true correlation $\rho$ or
	sample correlation $r_{\text{sample}}$. 
	\subsection{Theory}
%	\subsubsection{$S$ is a constant}
	
	\begin{theorem}\label{thm:teststatcor}
		Let $(X_j, Y_j), j = 1, \ldots, n$ be independent random vectors with mean and covariance structures specified in equation (\ref{eq:meanstruct}). If $(\bm a^T\bm X, \bm a^T\bm Y)$ is independent of $(S_X, S_Y)$, then the correlation of $T_X$ and $T_Y$ in equation (\ref{eq:teststat}) can be expressed as 
		\begin{equation}\label{eq:teststatcor}
			\rho_T(n) = \frac{ \rho E(S_X^{-1}S_Y^{-1}) + \frac{\bm a^T\bm \mu_X\cdot \bm a^T\bm \mu_Y}{\sigma_X\sigma_Y\bm a^T\bm a}\cov(S_X^{-1}, S_Y^{-1}) }{\sqrt{\left[E(S_X^{-2}) + \frac{(\bm a^T\bm \mu_X)^2}{\sigma_X^2\bm a^T\bm a}\var(S_X^{-1})\right]\left[E(S_Y^{-2}) + \frac{(\bm a^T\bm \mu_Y)^2}{\sigma_Y^2\bm a^T\bm a}\var(S_Y^{-1})\right]}}
		\end{equation}
	%	where $d_X = E(\bm a^T\bm \mu_X)$ and $d_Y = E(\bm a^T\bm Y)$.
	\end{theorem}
	\textbf{Proof:} Since samples are independent, we have 
	\begin{equation}\label{eq:testprepare}
	\begin{aligned}
		\cov(\bm a^T\bm X, \bm a^T\bm Y) &= \bm a^T \cov(\bm X, \bm Y)\bm a  = \rho\sigma_X\sigma_Y\bm a^T\bm a, \\
		\var(\bm a^T\bm X)&  = \sigma_X^2\bm a^T\bm a, \\
		 E(\bm a^T\bm X)^2& =(\bm a^T\bm \mu_X)^2 + \sigma_X^2\bm a^T\bm a, \\
		E[\bm (\bm a^T\bm X)(\bm a^T\bm Y)] &=E(\bm a^T\bm X)E(\bm a^T\bm Y) + \cov(\bm a^T\bm X, \bm a^T\bm Y)  	\\& = (\bm a^T\bm \mu_X)(\bm a^T\bm \mu_Y) + \rho \sigma_X\sigma_Y\bm a^T\bm a
	\end{aligned}
	\end{equation}
	Note that since $S_X$ is independent of $S_X$, we have 
	\begin{equation}\label{eq:testdenom1}
	\begin{aligned}
		\var(T_X) &= E\left[\left(\frac{\bm a^T\bm X}{S_X}\right)^2\right] - \left[E\left(\frac{\bm a^T\bm X}{S_X}\right)\right]^2\\
		& = E[\bm a^T\bm X]^2E[S_X^{-2}] - \left[E(\bm a^T\bm X)\right]^2\left[E(S_X^{-1})\right]^2\\
		& = \sigma_X^2\bm a^T\bm a E(S_X^{-2}) + (\bm a^T\bm \mu_X)^2\var(S_X^{-1})
	\end{aligned}
	\end{equation}
	Similarly, 
	\begin{equation}\label{eq:testdenom2}
	\var(T_Y)= \sigma_Y^2\bm a^T\bm a E(S_Y^{-2}) + (\bm a^T\bm \mu_Y)^2\var(S_Y^{-1})
	\end{equation}
	and
	\begin{equation}\label{eq:testnumerator}
	\begin{aligned}
		&\cov(T_X, T_Y) = E\left[\frac{ \bm a^T\bm X }{S_X^{-1}}\cdot\frac{ \bm a^T\bm Y }{S_Y^{-1}}\right] - E\left[\frac{\bm a^T\bm X}{S_X^{-1}}\right]E\left[\frac{\bm a^T\bm Y}{S_Y^{-1}}\right] \\
			 & = E[\bm (\bm a^T\bm X)(\bm a^T\bm Y)]\cdot E[S_X^{-1}S_Y^{-1}]-(\bm a^T\bm \mu_X)(\bm a^T\bm \mu_Y)E[S_X^{-1}]E[S_Y^{-1}]\\
			 & = [(\bm a^T\bm \mu_X)(\bm a^T\bm \mu_Y)+ \rho \sigma_X\sigma_Y\bm a^T\bm a]E[S_X^{-1}S_Y^{-1}]- (\bm a^T\bm \mu_X)(\bm a^T\bm \mu_Y)E[S_X^{-1}]E[S_Y^{-1}]
	\end{aligned}
	\end{equation}	
	The result follows by plugging equations (\ref{eq:testprepare})-(\ref{eq:testnumerator}) into equation (\ref{eq:popucor}).
	
	\begin{corollary}\label{thm:lineartransformation} 
		For any non zero $\bm a$, $\rho_T=\rho$ if $S_X$ and $S_Y$ are constant with respect to $\bm X, \bm Y$. 
	\end{corollary}
	\textbf{Proof}: When $S_X$ and $S_Y$ are constants, $\cov(S_X^{-1}, S_Y^{-1})$, $\var(S_X^{-1}) $ and $\var(S_Y^{-1})$ are all 0, and equation (\ref{eq:teststatcor}) reduces to 
	\begin{equation}
		\rho_T(n) = \frac{\rho E(S_X^{-1}S_Y^{-1})}{\sqrt{E(S_X^{-2})E(S_Y^{-2})}} = \rho.
	\end{equation}
	\textbf{Note:} Corollary \ref{thm:lineartransformation} states that test statistics correlation and expression level correlation are equal under linear transformation of $\bm X$ and $\bm Y$. In a two group comparison, Let $n = n_1 + n_2$ where $n_1, n_2$ are the sample sizes for the two groups, and 
	\begin{equation}\label{eq:contrast}
	\bm a  = (\underbrace{\frac{1}{n_1}, \ldots, \frac{1}{n_1}}_{n_1}, \underbrace{-\frac{1}{n_2}, \ldots, -\frac{1}{n_2}}_{n_2})^T
	\end{equation}
 be a contrast. If we set $S_X=1$, then $T_X$ corresponds to mean difference between the treatment and the control group; instead, if $S_X = \sigma_X\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$ where $\sigma_X$ is known, then $T_X$ corresponds to the statistic for two sample $z$-test. Therefore, $\rho_T=\rho$ if we use mean difference or $z$-value as test statistic.
	
	Next, if we assume that $(S_X, S_Y)$ is a function of $(\bm X, \bm Y)$, then the test statistics correlation in equation (\ref{eq:teststatcor}) can be expressed as  
	\begin{equation}
		\rho_T(n) = \frac{ \frac{E(S_X^{-1}S_Y^{-1})}
			{\sqrt{\var(S_X^{-1})\var(S_Y^{-1})}}\rho + \frac{(\bm a^T\bm \mu_X)(\bm a^T\bm \mu_Y)}{\sigma_X\sigma_Y\bm a^T\bm a} \rho_s	
		}{\sqrt{\left[ \frac{E(S_X^{-2})}{\var(S_X^{-1})} + \frac{(\bm a^T\bm \mu_X)^2}{\sigma_X^2\bm a^T\bm a}\right]\left[ \frac{E(S_Y^{-2})}{\var(S_Y^{-1})} + \frac{(\bm a^T\bm \mu_Y)^2}{\sigma_Y^2\bm a^T\bm a}\right]}} 
	\end{equation}
	where 
	\begin{equation}
	 \rho_s = \frac{\cov(S_X^{-1},S_Y^{-1})}{\sqrt{\var(S_X^{-1})\var(S_Y^{-1})}}
	\end{equation}
	SAY SOMETHING HERE....
	
	In gene expression analysis, a commonly used statistic is the two sample $t$-statistic (REF).
	For a two-factor experiment, we let $n_1$ and $n_2$ be the sample size for each treatment. The mean expression level is $\bm \mu_j = (\mu_X, \mu_Y)$  for the first treatment (i.e., $j = 1, \ldots, n_1$), and  $\bm \mu_j = (\mu_X,  \mu_Y)^T  + ( \Delta_X,\Delta_Y)^T$ for the second treatment (i.e., $j = n_1 + 1, \ldots, n_1 + n_2$). 
	
	In the case of two sample $t$-test with equal variance, the contrast $\bm a$ is defined in equation (\ref{eq:contrast}), giving the test statistic 
	\begin{equation}\label{eq:tx}
		T_X= \frac{\bar{X}_1- \bar{X}_2}{S_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}
	\end{equation}
	where $S_p$ is the pooled variance
	 \begin{equation}\label{eq:tstatform}
	 \begin{aligned}
	 &S_{X}^2 = \frac{(n_1-1)S_{X, 1}^2 + (n_2 -1)S_{X,2}^2}{n_1 + n_2 -2}. \\
	% 	&S_{X, 1}^2 =  \frac{\sum_{j=1}^{n_1}(X_j - \bar{X}_1)^2}{n_1 -1}, ~~S_{X, 2}^2 =  \frac{\sum_{j=n_1 +1}^{n_1 + n_2}(X_j - \bar{X}_2)^2}{n_2 -1},\\
	 \end{aligned}
	 \end{equation}
 Similarly, we obtain $T_Y$ by replacing the subscript ``$X$" in equations (\ref{eq:tx}) and (\ref{eq:tstatform}). Under normal distribution assumption, we have the following theorem for two sample $t$-test with equal variance:
	\begin{theorem}\label{thm:tstat}
		Let $(X_i, Y_i), i = 1, \ldots, n_1 + n_2$ follow a bivariate normal distribution with mean and covariance specified by equation (\ref{eq:meanstruct}). If $T_X$ and $T_Y$ are statistics for equal-variance two-sample $t$-test, then 
		\begin{equation}\label{eq:ttestcor}
	\begin{aligned}
	 \cor(T_X, T_Y) =   
	 \frac{\frac{\Delta_X\Delta_Y}{\sigma_X\sigma_Y}C \rho_{s}+ \rho B
		+ \rho_{s}\rho(A-B)}{\sqrt{\left[ \frac{\Delta_X^2}{\sigma_X^2}C + A\right]\left[\frac{\Delta_Y^2}{\sigma_X^2}C +   A\right]}}
	\end{aligned}
		\end{equation}
	where 
			 \begin{equation}\label{eq:AandB}
			 	 \begin{aligned}
			 	 A & = \frac{n_1 + n_2-2}{n_1 + n_2-4}, ~~B =
			 	 \frac{(\frac{n_1 + n_2 -2}{2})\Gamma^2(\frac{n_1 + n_2 -4}{2} + \frac{1}{2})}{\Gamma^2(\frac{n_1+ n_2 -2}{2})}, \\
			 	 \rho_s & = \cor(S_X^{-1}, S_Y^{-1}), ~~ 
			 	 C = \frac{(n_1 + n_2)(A-B)}{(2 + n_1n_2^{-1} + n_1n_2^{-1})}.
			 	 \end{aligned}
			 \end{equation}	 
	\end{theorem}
	The proof of Theorem \ref{thm:tstat} is presented in Section \ref{section:testcormethod}. Next we give the limit of $\cor(T_X, T_Y)$.
	\begin{theorem}\label{thm:rholimit}
	  	If there exists positive constant $M_1$ and $M_2$, such that $M_1 \leq n_1n_2^{-1}\leq M_2$, then
	   \begin{equation}\label{eq:limitT}
	   \rho_T=\lim\limits_{n_1 + n_2 \rightarrow \infty} \cor(T_X, T_Y) = \frac{\rho(1  +
	   	\beta\frac{\Delta_X\Delta_Y}{\sigma_X\sigma_Y}\rho)}{\sqrt{  \left[ 1 +\beta\frac{\Delta_X^2}{\sigma_X^2}\right]\left[ 1 + \beta\frac{\Delta_Y^2}{\sigma_Y^2}\right]}}
	   \end{equation}
	   where %$\rho_{s}$ is defined in equation (\ref{eq:AandB}) and 
	   $\beta = \lim\limits_{n_1 + n_2 \rightarrow \infty}C = (4 + 2n_1^{-1}n_2 + 2n_1n_2^{-1})^{-1}$.
	\end{theorem}
	Theorem \ref{thm:rholimit} says that as long as $n_1$ and $n_2$ grow proportionally, the limit of $\cor(T_X, T_Y)$ is a function of population covariance $\bm \Sigma$, the true mean difference $\bm \Delta$ and the ratio $n_1/n_2$. We have the following observations:
	\begin{enumerate}
		\item If both test are true null (i.e., $\bm \Delta = \bm 0$), then $\rho_T = \rho$.
		\item If one test is true null, then $\rho_T$ is proportional to and smaller in absolute value than $\rho$ (i.e., $|\rho_T|< |\rho|$).
		\item If both tests are true alternative (i.e., $\bm \Delta \neq \bm 0$), then $\rho_T\neq \rho$ in general. Specifically,
			\begin{enumerate}
				\item  when $\Delta_X\Delta_Y >0$ (i.e., both genes are DE towards the same direction), we have $\rho_T>\rho$ for $\rho <0$ and $0 \leq \rho_T \leq\rho$ for $\rho \geq 0$.
				\item when $\Delta_X\Delta_Y <0$ (i.e., genes are DE towards different directions), we have
				$\rho <\rho_T<0$ for $\rho <0$ and $\rho_T<\rho$ for $\rho>0$.
			\end{enumerate}
		Therefore in either case, we have $|\rho_T| \leq |\rho|$. 
		\end{enumerate}
		
	For a two sample $t$ test with equal variance, we note that $|\rho_T| \leq |\rho|$. In other words, $T_X$ and $T_Y$ are never ``more correlated" than $X$ and $Y$ are. It's also interesting to note that when both genes are DE, $\rho_T=0$ at $\rho =-\frac{\sigma_X\sigma_Y}{\beta\Delta_X\Delta_Y} $ and $\frac{\sigma_X\sigma_Y}{\beta\Delta_X\Delta_Y} \in (-1, 1)$.
			
	 In addition, we note that if $n_1/n_2 \rightarrow 0$ or $\infty$, then $\beta = 0$ and we have $\rho_T = \rho$. That is, when sample size of one group is not proportional to that of the other, $\cor(T_X, T_Y)$ will converge to $\rho$ regardless of whether the tests are under the null or not. 	
%	 When $\bm \Delta = \bm 0$ then $\rho_T = \rho$; but when $\bm \Delta \neq \bm 0$, then $\rho_T \neq \rho$ in general. % In the next section, we will discuss about it in further detail. 
%	\begin{corollary} 
%	If $\bm \Delta  = (\Delta_X, \Delta_Y)= \bm 0$, and there exists a positive number $M$, such that  $n_1n_2^{-1}\leq M$ and $n_1n_2^{-1}\leq M$,  then $\cor({T_X, T_Y})\rightarrow \rho$ as $n_1 + n_2 \rightarrow \infty$. 
%	\end{corollary}
%	Proof:  If null is true for both test or $\bm \Delta  = 0$, then equation (\ref{eq:ttestcor}) reduces to
%	\begin{align}\label{CalculateTCor}
%		\cor(T_X, T_Y) = \left[\rho_s \cdot 1 + (1-\rho_s)\frac{B}{A}\right]\rho
%	\end{align}
%	The term in the square bracket is a weighted average of 1 and $\frac{B}{A}$, with the latter
%	converging to 1 as $n_1 + n_2$ grows to infinity. Therefore $\lim\limits_{n_1 + n_2\rightarrow\infty} \cor({T_X, T_Y}) = \rho$.
%	\begin{corollary} 
%		If $\bm \Delta = (\Delta_X, \Delta_Y)\neq \bm 0$, then  $\cor({T_X,
%			T_Y})$ does not converge to $\rho$ in general.
%	\end{corollary} 
%	The result immediately follows from lemma  (\ref{lemmaLimit}) in appendix.
%	Depending on the underlying value of $\bm\Delta$ (DE or not DE, up-regulated or down-regulated if DE) and covariance $\bm \Sigma$,
%	$\rho_T$ might be far from $\rho$ in different ways. 

%	In the Methods section, Lemma \ref{thm:invScorlimit} states that
	
	
	\subsection{Simulation}
	We perform simulations to evaluate the correlations between test statistics and those between expression levels under two sample $t$-test. We simulate the expression data from normal distributions. Specifically, we let $(X, Y)$ be the expression levels of genes $X$ and $Y$, and
	\begin{equation}
		\begin{aligned}
			&\left( \begin{array}{c}
				X_{j_1}\\
				Y_{j_1}\\
			\end{array}\right)
			\sim N\left[
			\left(\begin{array}{c}
				0\\
				0\\
			\end{array} \right), 
			\left(
			\begin{array}{cc}
				1 &\rho \sqrt{1\cdot 3} \\
				\rho \sqrt{1\cdot 3} & 	3 \\
			\end{array}
			\right)
			\right] \\
			& \left( \begin{array}{c}
				X_{j_2}\\
				Y_{j_2}\\
			\end{array}\right)
			\sim N\left[
			\left(\begin{array}{c}
				\Delta_X\\
				 \Delta_Y\\
			\end{array} \right), 
			\left(
			\begin{array}{cc}
				1 &\rho \sqrt{1\cdot 3} \\
				\rho \sqrt{1\cdot 3} & 	3 \\
			\end{array}
			\right)
			\right] 
		\end{aligned}
	\end{equation}
	where $j_1 = 1,\ldots, n_1$ and $j_2 = n_1 + 1, \ldots, n_1 + n_2$. In this simulation setting, we set both $n_1$ and $n_2$ to be 100.  For each given $\rho$, we consider these $n=200$ pairs of $(X, Y)$
	as observations from one \textit{simulated} experiment. Out of this experiment, we calculate $q = (T_X, T_Y, r_{XY})$ where $T_X$ and $T_Y$ are the test statistics for gene $X$ and gene $Y$ respectively using 
	two-sample $t$-test for equal variance procedure, and $r_{XY}$ is the sample correlation after the treatment effect is removed. We replicate the simulated
	experiment for $B=1000$ times, resulting in a matrix $\bm Q_{1000\times 3}$. We take the correlation between the first and the second columns of $\bm Q$ as an estimate for test statistics correlation	$r_\text{statistics}$, and 
	the mean of the third column as an estimate of sample correlation $r_{\text{sample}}$. 
	\citet{fisher1915frequency} proved that sample correlation is a consistent estimator for underlying true 
	correlation, therefore
		$r_\text{statistics}$ and $r_{\text{sample}}$ should reflect the true correlation between $T_X$ and $T_Y$ and 
		that between $X$ and $Y$ respectively. 
		We increase $\rho$ from $-0.99$ to $0.99$ by fixed step size $0.01$, and examine the relationship between $r_\text{statistics}$ and $r_{\text{sample}}$ under four different cases:
	\begin{enumerate}
		\item[a)] No DE genes (i.e., $\Delta_X = \Delta_Y  =0$);
		\item[b)] One gene is DE and the other is not (i.e, only one of $\Delta_X$ and $\Delta_Y$ is 0); in the simulation we set $\Delta_X = 0$ and $\Delta_Y=5$;
		\item[c)] DE towards the same direction (i.e., $\Delta_X\Delta_Y>0$); in the simulation we set $\Delta_X = 2$ and $\Delta_Y=5$;
		\item[d)] DE towards opposite directions (i.e., $\Delta_X\Delta_Y <0$); in the simulation we set $\Delta_X = 2$ and $\Delta_Y=-5$.
		\end{enumerate}
		
	In Figure \ref{fig:tstat}, we plot $r_\text{statistics}$ and $r_{\text{sample}}$ against the underlying true population correlation $\rho$. Note that in all cases, while $r_{\text{sample}}$ is a consistent estimator of $\rho$, $r_\text{statistics}$ might be very different from $\rho$ and thus from $r_\text{sample}$.  
	In case a) where no gene is DE, $r_\text{statistics}$ and $r_{\text{sample}}$ are almost equal, and both converge the true correlation $\rho$. However, as long as DE effect exists, there is a discrepancy between $r_\text{statistics}$ and $\rho$. In case b) where only one gene is DE, 
	 the magnitude of $r_\text{statistics}$ is proportional to, and smaller in absolute value than $\rho$.
	% and according to equation (\ref{eq:ttestcor}), the magnitude of $r_\text{statistics}$ has to do with the signal-to-noise ratio $\Delta/\sigma$
	It is more interesting to note that $r_\text{statistics}$ is not monotone with respect to $\rho$ when both genes are DE. If genes are DE towards the same direction as in the case of c),  $r_\text{statistics}$ first decreases from a positive value to 0, and continues to decrease until it reaches the minimum (a negative value), and then gradually increases to 1, as $\rho$ grows from $-1$ to $1$. When genes are DE towards opposite directions like in case d), however, the trend is reversed from that in the case of c).	This set of simulation results is reflected in the test statistics correlation formula of equation (\ref{eq:limitT}). 
	
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=6.5cm, height= 6.5cm]{Figures/case1.eps}
		\includegraphics[width=6.5cm, height= 6.5cm]{Figures/case2.eps}
		\includegraphics[width=6.5cm, height= 6.5cm]{Figures/case3.eps}
		\includegraphics[width=6.5cm, height= 6.5cm]{Figures/case4.eps}
	\caption{Plots for estimates of sample/test statistics correlation against true population correlations. For each of the simulation settings a)--d), the test statistics are calculated using two sample $t$-test with equal variance, and the correlations are calculated by equation (\ref{eq:samplecor}). }
		\label{fig:tstat}
	\end{figure}

	
	
	
	
	
	\section{Method}\label{section:testcormethod}
	
	\begin{lemma}
		Sample correlation coefficient is a consistent estimator for $\rho$, 
		\[\sqrt{n}(r_{\text{sample}} - \rho ) \stackrel{D}{\rightarrow}N\left(0, (1-\rho^2)^2\right).\]
	\end{lemma}
	The proof of lemma 1 can be found in \citet{fisher1915frequency}. \\
	%$\rho({G_1, G_2})$ and $\frac{{\rho}_X + {\rho}_Y}{2}$ are asymptotically equivalent. \\
	
	To prove Theorem \ref{thm:tstat}, it is useful to note that $\bm U$ is independent of $\bm S$,
	following from Lemmas \ref{lemmabiChisq} and \ref{lemmaIndep}.
	\begin{lemma}\label{lemmabiChisq}
		Let $(X_{j}, Y_{j}), j=1 \ldots,  n$ be independent random variables satisfying equation (\ref{eq:indepsamples}),
		then $\bm W = (W_{X},W_{Y}) =(\frac{(n -1)S_{X}^2}{\sigma_X^2}, \frac{(n-1)S_{Y}^2}{\sigma_Y^2})$ 
		follows a \textbf{bivariate chi square distribution} with density 
		\begin{equation}\label{biChisq}
			\begin{aligned}
				f(w_x, w_y) & = \frac{2^{-n}(w_xw_y)^{(n-3)/2}e^{-\frac{w_x +
							w_y}{2(1-\rho^2)}}}{\sqrt{\pi}\Gamma(\frac{n}{2})(1-\rho^2)^{(n-1)/2}} \times \\
				& \sum_{k=0}^{\infty}[1 +
				(-1)^k]\left(\frac{\rho\sqrt{w_xw_y}}{1-\rho^2}\right)^k\frac{\Gamma(\frac{k+1}{2})}{k!\Gamma(\frac{k
						+ n}{2})}
			\end{aligned}
		\end{equation}
		for $n>3$ and $-1<\rho < 1$.
	\end{lemma}
	For proof of Lemma \ref{lemmabiChisq}, interested readers are referred to \citet{joarder2009moments}.
	It immediately follows from Lemma \ref{lemmabiChisq} that $\bm W_1 = (\frac{(n_1 -1)S_{X, 1}^2}{\sigma_X^2}, \frac{(n_1-1)S_{Y, 1}^2}{\sigma_Y^2})$ follows bivariate chi-square distribution with degree of freedom $n_1-1$, where $S_{X_,1}$ and $S_{X, 2}$ are defined in equation (\ref{eq:samplevariance}). Similarly, $\bm W_2 =(\frac{(n_2 -1)S_{X, 2}^2}{\sigma_X^2}, \frac{(n_2-1)S_{Y, 2}^2}{\sigma_Y^2})$ follows a bivariate chi-square distribution with degree of freedom $n_2-1$.  Note that $\bm W_1$ and $\bm W_2$ are independent since the samples are independent. 
	
	\begin{lemma}\label{lemmaIndep}
		$\bm U =(U_X, U_Y)$ is independent of $\bm S = (S_X ,S_Y)$, where $\bm U$ and $\bm S$ are defined in
		equation (\ref{eq:tstatform}).
	\end{lemma}
	\textbf{Proof}: By Lemma \ref{lemmabiChisq}, the density function of $
	\bm W_1 + \bm W_2$ only involves $\sigma^2_X, \sigma^2_Y, \rho$ and sample size $n_1, n_2$, therefore
	we can denote its density by some function $g(\sigma^2_X, \sigma^2_Y, \rho,
	n_1 + n_2)$. Note that $\bm S^2 = \frac{(\sigma_X^2, ~\sigma^2_Y)}{n_1 +n_2 -2}(\bm W_1 + \bm W_2)^T $
	is a linear transformation of $\bm W_1 + \bm W_2$, so its density also can be expressed in terms of $\sigma^2_1, \sigma^2_2, \rho, n_1, n_2$. Therefore $\bm S = (S_X ,S_Y)$ is an ancillary statistic for $\bm \Delta$. On the other hand, it can
	be shown that $\bm U =(U_X, U_Y)$ is a complete sufficient statistic for $\bm \Delta$. It follows by
	Basu's theorem that $\bm U$ and $\bm S$ are independent. 
	
	
	Lemma \ref{lemmaIndep} implies that  $U_XU_Y$ is also independent of $S_X^{-1}S_Y^{-1}$, and
	therefore $E(\frac{U_X}{S_X} \cdot\frac{U_Y}{S_Y})$ can be expressed as
	$E(U_XU_Y)E(S_X^{-1}S_Y^{-1})$. Additionally, if we know $\cor(S_X^{-1}, S_Y^{-1})$,
	then the $t$-test statistics correlation can be accurately represented. 
	
	\textbf{Proof of theorem (\ref{thm:tstat})} \\
	First note that
	\begin{align*}
		\cov(T_X, T_Y) &= E(T_XT_Y) - E(T_X)E(T_Y) \\
		%& = E(c_0\frac{U_1}{S_1} \cdot c_0\frac{U_2}{S_2}) - E(c_0\frac{U_1}{S_1})E( c_0\frac{U_2}{S_2}) \\
		& = \frac{1}{c_0^2} \left[E(U_XU_Y)E(S_X^{-1}S_Y^{-1}) - E(\frac{U_X}{S_X})E( \frac{U_Y}{S_Y})\right]   
		~~~~~~~~~\text{ (by lemma \ref{lemmaIndep})}
	\end{align*}
	where $c_0 = \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$ and $\var(T_X) = \var(\frac{U_X}{c_0S_X})=
	\frac{1}{c_0^2}\var(\frac{U_X}{S_X})$. 
	Note that 
	\begin{equation}\label{eq:Tcorrelation}
		\begin{aligned}
			\cor(T_X, T_Y) & = \frac{\cov(T_X, T_Y) }{\sqrt{\var(T_X) \var(T_Y) }} \\
			& = \frac{E(U_XU_Y)E(S_X^{-1}S_Y^{-1}) - E(\frac{U_X}{S_X})E(
				\frac{U_Y}{S_Y})}{\sqrt{\var(\frac{U_X}{S_X})\var(\frac{U_Y}{S_Y})}} 
		\end{aligned}
	\end{equation}
	We need to calculate $E(U_XU_Y)$, $E(S_X^{-1}S_Y^{-1})$, $ E(\frac{U_i}{S_i})$ and
	$\var(\frac{U_i}{S_i})$ for $i =X, Y$. 
	\begin{enumerate}
		\item Note that $U_i\sim N\left(\Delta_i, \sigma_i^2(\frac{1}{n_1} + \frac{1}{n_2})\right), i=X, Y$. 
		\begin{equation}\label{eq1}
			\begin{aligned}
				E(U_XU_X)&= \cov(U_X, U_Y) + E(U_X)E(U_Y) \\
				&= \rho\sigma_X\sigma_Y\left(\frac{1}{n_1} + \frac{1}{n_2}\right) +
				\Delta_X\Delta_Y
			\end{aligned} 
		\end{equation}
	
		\item Since $\frac{(n_1-1)S_{X}^2}{\sigma_X^2}$ and $\frac{(n_2-1)S_{Y}^2}{\sigma_Y^2}$ are
		independent and follow $\chi^2(n_1-1)$ and $\chi^2(n_2 -1)$ respectively, , we have $W_{X}=\frac{(n_1 + n_2 -2)S_X^2}{\sigma_X^2}\sim
		\chi^2(n_1 + n_2-2)$. It can be shown that 
		\[E(W_{X}^k)= \frac{2^k\Gamma(\frac{n_1 + n_2 -2}{2}+k)}{\Gamma(\frac{n_1 + n_2 -2}{2})}\] 
		Therefore 
		\begin{equation}
		\begin{aligned}
		E\left(S_X^{-1}\right) =
	 \frac{\sqrt{B}}{\sigma_X},		~~~\var\left(S_X^{-1}\right) = \frac{A-B}{\sigma_X^2}
		\end{aligned}
		\end{equation}
			Note that $\rho_s = \cor(S_X^{-1}, S_Y^{-1})$, we have 
			\begin{equation}\label{eq2}
			\begin{aligned}
			E(S_X^{-1}S_Y^{-1})  &= E(S_X^{-1})E(S_Y^{-1}) + \rho_s
			\sqrt{\var(S_X^{-1})\var(S_Y^{-1})} \\
			& = \frac{B}{\sigma_X\sigma_Y} + \rho_s
			\frac{A-B}{\sigma_X\sigma_Y}
			\end{aligned}
			\end{equation}
	
		\item $U_i\sim N\left(\Delta_i, \sigma_i^2(\frac{1}{n_1} + \frac{1}{n_2})\right)$ and $\frac{(n_1 + n_2 -2)S_i^2}{\sigma_i^2} \sim
		\chi^2(n_1 + n_2-2)$ and by Lemma \ref{lemmaIndep}  $U_i$ and $\frac{(n_1 + n_2 -2)S_i^2}{\sigma_i^2}$ are independent for $i = X, Y$, we have 
		\begin{equation}
			\frac{\frac{U_i-\Delta_i}{\sigma_i\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}}{\frac{(n_1 + n_2-2)S_i^2}{\sigma_i^2}/(n_1 + n_2 -2)}  =
			\frac{U_i-\Delta_i}{S_i\sqrt{\frac{1}{n_1 } + \frac{1}{n_2}}}\sim t(n_1 + n_2-2)
		\end{equation}	
		It follows from 
		\begin{equation}
			E\left(\frac{U_i-\Delta_i}{S_i\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}\right)=0, ~~ \text{Var}\left(\frac{U_i-\Delta_i}{S_i\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}\right) = \frac{n_1 + n_2-2}{n_1 + n_2-4}
		\end{equation}
	 that
	 \begin{equation}\label{eq3}
	\begin{aligned}
	E\left(\frac{U_i}{S_i}\right) &= \frac{\Delta_i}{\sigma_i}\sqrt{B} \\
	\var\left(\frac{U_i}{S_i}\right)&=A\left(\frac{1}{n_1} + \frac{1}{n_2}\right) + \frac{\Delta_i^2}{\sigma_i^2}(A-B)
	\end{aligned}
	 \end{equation}
	\end{enumerate}
	Finally,  the test statistics correlation (\ref{eq:ttestcor}) is obtained by plugging
	equations (\ref{eq1}--\ref{eq3}) into equation (\ref{eq:Tcorrelation}).
	
	Up to now we have obtained an exact expression for $\cor(T_X, T_Y)$, which depends not only on
	the sample size $n_1$ and $n_2$, but also on $\Delta/\sigma$, the relative magnitude of DE. The rest of this section
	discusses asymptotic  property of $\cor(T_X, T_Y)$ for large sample size.
	\begin{lemma}\label{lemmaLimit}
	If there exists a positive number $M$, such that  $n_1n_2^{-1}\leq M$ and $n_1n_2^{-1}\leq M$, then the following results hold:
		\begin{enumerate}
			\item $\lim\limits_{n_1 + n_2\rightarrow \infty} A = 1$.
			\item $\lim\limits_{n_1 + n_2\rightarrow \infty} B = 1$.
			\item $\lim\limits_{n_1 + n_2\rightarrow \infty} C = \beta.$
		\end{enumerate}
	where  $A, B$ and $C$ are defined in equation (\ref{eq:AandB}), and $\beta= (4 + n_1n_2^{-1} + n_1^{-1}n_2)^{-1}$. 
	\end{lemma}
	\textbf{Proof}: Note that 
	\begin{equation}
	B = 
	\begin{cases}
	\frac{(k-1)\Gamma^2(k- \frac{3}{2})}{\Gamma^2(k-1)},& \text{if } n_1 + n_2 = 2k, k\geq 2 \\
	~\\
	 \frac{(k-\frac{1}{2})\Gamma^2(k- 1)}{\Gamma^2(k-\frac{1}{2})},& \text{if } n_1 + n_2 = 2k+1, k\geq 2 \\
	\end{cases}
	\end{equation}
	We will use second order Stirling's formula,
	\begin{align}\label{Stirling1}
		k! \approx \sqrt{2\pi k}\left(\frac{k}{e}\right)^k(1 + \frac{1}{12k})
	\end{align}
	Using Stirling's formula (\ref{Stirling1}) and  $\Gamma(k + \frac{1}{2}) =
	\frac{(2k)!}{4^kk!}\sqrt{\pi}$, it can be shown that 
	\begin{equation}\label{eq:Bapprox}
			B \approx  
		\begin{cases}
			\frac{(k-1)(k-2)(k-2 + \frac{1}{24})^2}{(k-2 + \frac{1}{12})^4},& \text{if } n_1 + n_2 = 2k, k\geq 2 \\
			  ~\\
			\frac{(k-\frac{1}{2})(k - 1 + \frac{1}{12})^4}{(k-1+ \frac{1}{24})^2(k-1)^3},& \text{if } n_1 + n_2 = 2k+1, k\geq 2 \\
		\end{cases}
	\end{equation}
	It can also be shown using equation (\ref{eq:Bapprox}) that
	\begin{equation}\label{eq:AminusB}
		A- B \approx  
		\begin{cases}
		\frac{\frac{1}{4}(k-1)(k-2)^3 + o((k-2)^4)}{(k-2)(k-2 + \frac{1}{12})^4},& \text{if } n_1 + n_2 = 2k, k\geq 2 \\
		~\\
		\frac{\frac{1}{4}(k-1)^3(k-\frac{1}{2})(k-3) + o((k-1)^4)}{(k-\frac{3}{2})(k-1+ \frac{1}{24})^2(k-1)^3},& \text{if } n_1 + n_2 = 2k+1, k\geq 2 \\
		\end{cases}
	\end{equation}
	And the results immediately follow.
	
	\begin{lemma}\label{thm:invScorlimit}
		Let $\bm Z_j = (X_j, Y_j), j = 1, \ldots, n$ be i.i.d. random variables under the two sample $t$-test for equal variance setting, 
%	  \begin{equation}\notag
%	   Z_j \sim N\left[ \left(
%	   \begin{array}{c}
%	     \mu_X\\
%	     \mu_Y \\
%	   \end{array} \right), 
%	   \left(
%	   \begin{array}{cc}
%	   \sigma_X^2	& \rho\sigma_{X}\sigma_Y\\
%	   	  \rho\sigma_{X}\sigma_Y & \sigma_Y^2\\
%	   	  \end{array}
%	   \right)
%	   \right] 
%	  \end{equation}
		with covariance structure given by equation (\ref{eq:meanstruct}). Then we have
		$\lim\limits_{n\rightarrow\infty}\rho_s = \rho^2$.
	\end{lemma}
	\textbf{Proof}: Let's first look at samples $j=1, \ldots, n_1$. Note that 
	 \begin{equation}
	     S_{X,1}^2= \frac{1}{n_1}\sum_{j=1}^{n_1}(X_j -\bar{X}_1)^2
	 \end{equation}
	 is the \textit{maximum likelihood estimator} (MLE) for $\bm \sigma_X^2$. Because MLEs are consistent and asymptotically unbiased with respect to the true parameter, it can be shown that 
		\begin{equation}\label{eq:sampVarasypm}
	 	\begin{aligned}
	 	E[S_{X, 1}^2] = \sigma_X^2,&~E[S_{Y, 1}^2] = \sigma_Y^2,  \\
	 	\var[S_{X, 1}^2] \rightarrow \frac{2\sigma_X^4}{n_1},~\var[S_{Y, 1}^2] \rightarrow \frac{2\sigma_Y^4}{n_1},&
	 	~\cov(S_{X, 1}^2, S_{Y, 1}^2) \rightarrow \frac{2\rho^2\sigma_X^2\sigma_Y^2}{n_1} \\
	 	\end{aligned}
	 	\end{equation} 
%	 	\begin{equation}\label{eq:sampVarasypm}
%	 	\begin{aligned}
%	 	& \sqrt{n_1}\left[\left( \begin{array}{c}
%	 	S_{X, 1}^2\\
%	 	S_{Y, 1}^2\\
%	 	\end{array}\right)
%	 	-
%	 	\left( \begin{array}{c}
%	 	\sigma_X^2\\
%	 	\sigma_Y^2\\
%	 	\end{array}\right)
%	 	\right]
%	 	\stackrel{d.}{\longrightarrow} 
%	 	N\left[
%	 	\left(\begin{array}{c}
%	 	0\\
%	 	0\\
%	 	\end{array} \right), 
%	 	2\left(
%	 	\begin{array}{cc}
%	 	\sigma_X^4 &\rho^2\sigma_X^2\sigma_Y^2 \\
%	 	\rho^2\sigma_X^2\sigma_Y^2  &\sigma_Y^4 \\
%	 	\end{array}
%	 	\right)
%	 	\right] \\
%	 		& \sqrt{n_2}\left[\left( \begin{array}{c}
%	 		S_{X, 2}^2\\
%	 		S_{Y, 2}^2\\
%	 		\end{array}\right)
%	 		-
%	 		\left( \begin{array}{c}
%	 		\sigma_X^2\\
%	 		\sigma_Y^2\\
%	 		\end{array}\right)
%	 		\right]
%	 		\stackrel{d.}{\longrightarrow} 
%	 		N\left[
%	 		\left(\begin{array}{c}
%	 		0\\
%	 		0\\
%	 		\end{array} \right), 
%	 		2\left(
%	 		\begin{array}{cc}
%	 		\sigma_X^4 &\rho^2\sigma_X^2\sigma_Y^2 \\
%	 		\rho^2\sigma_X^2\sigma_Y^2  &\sigma_Y^4 \\
%	 		\end{array}
%	 		\right)
%	 		\right] 
%	 	\end{aligned}
%	 	\end{equation}
	Note that
		\begin{equation}\label{eq:Slinearcomb}
		\begin{aligned}
		&\left( \begin{array}{c}
		S_{X}^2\\
		S_{Y}^2\\
		\end{array}\right)
		= 
		a_1\left( \begin{array}{c}
		S_{X, 1}^2\\
		S_{Y,1}^2\\
		\end{array}\right)
		+
		a_2\left( \begin{array}{c}
		S_{X, 2}^2\\
		S_{Y, 2}^2\\
		\end{array}\right)
		\end{aligned}
		\end{equation}
		where 
		\[ 	n = n_1 + n_2, ~~a_1 = \frac{n_1 -1}{n-2}, ~~a_2 = \frac{n_2 -1}{n-2} \]
	We have by Slutsky's theorem 
%	\begin{equation}\label{eq:poolsampVarasypm}
%	\begin{aligned}
%	&E[S_X^2] = \sigma_X^2, \var[S_X^2] \rightarrow \frac{2\sigma_X^4}{n}\\
%	&\cov(S_X^2, S_Y^2) \rightarrow \frac{2\rho^2\sigma_X^2\sigma_Y^2}{n} \\
%	&E[S_Y^2] = \sigma_Y^2, \var[S_Y^2] \rightarrow \frac{2\sigma_Y^4}{n}\\
%	\end{aligned}
%	\end{equation}
		\begin{equation}\label{eq:poolsampVarasypm}
		\begin{aligned}
		& \sqrt{n}\left[\left( \begin{array}{c}
		S_{X, 1}^2\\
		S_{Y, 1}^2\\
		\end{array}\right)
		-
		\left( \begin{array}{c}
		\sigma_X^2\\
		\sigma_Y^2\\
		\end{array}\right)
		\right]
		\stackrel{d.}{\longrightarrow} 
		N\left[
		\left(\begin{array}{c}
		0\\
		0\\
		\end{array} \right), 
		2\left(
		\begin{array}{cc}
		\sigma_X^4 &\rho^2\sigma_X^2\sigma_Y^2 \\
		\rho^2\sigma_X^2\sigma_Y^2  &\sigma_Y^4 \\
		\end{array}
		\right)
		\right] 
		\end{aligned}
		\end{equation}
		If we let $g(x) = x^{-\frac{1}{2}}$, and apply $\delta$-method to equation (\ref{eq:poolsampVarasypm}), we obtain
		\begin{equation}\label{eq:invSasymp}
		\begin{aligned}
		& \sqrt{n}\left[\left( \begin{array}{c}
		S_X^{-1}\\
		S_Y^{-1}\\
		\end{array}\right)
		-
		\left( \begin{array}{c}
		\sigma_X^{-1}\\
		\sigma_Y^{-1}\\
		\end{array}\right)
		\right]
		\stackrel{d.}{\longrightarrow} 
		N\left[
		\left(\begin{array}{c}
		0\\
		0\\
		\end{array} \right), 
		\frac{1}{2}\left(
		\begin{array}{cc}
		\sigma_X^{-2} &\rho^2\sigma_X^{-1}\sigma_Y^{-1} \\
		\rho^2\sigma_X^{-1}\sigma_Y^{-1}  &\sigma_Y^{-2} \\
		\end{array}
		\right)
		\right] 
		\end{aligned}
		\end{equation}
	It follows from equation (\ref{eq:invSasymp}) that $\cor(S_X^{-1}, S_Y^{-1}) \rightarrow \rho^2$.
	
	
	
	\section{Conclusion}
	
	\textbf{State the major findings} \\
	This article discusses the relationship between sample correlation coefficients $r_{\text{sample}}$ 
	(after treatment effects removed) and test statistics correlation $r_\text{statistics}$ in a two
	group comparison setting. We proved that under normal distribution assumption, $r_\text{statistics}$
	and $r_\text{sample}$ have a perfect positive correlation for two sample $z$ test. However, for two
	sample $t$-test this correspondence does not hold in general, unless the null in (\ref{hypotheses})
	is true for all the tests considered. The results for two sample $t$-test can be applied to the case
	of two group mean comparison under Poisson regression, as shown by simulation. Consequently, that
	estimating  $r_\text{statistics}$ by $r_{\text{sample}}$ after nullifying treatment effects can not
	be taken for granted.
	
	\textbf{State the practical meaningness of the findings}\\
	In gene expression analysis, cares need to be taken when estimating test statistics correlation from
	sample correlation. For microarray data, two sample $t$ test (\cite{efron2007correlation},
	\cite{barry2008statistical}) or its moderated version \citep{wu2012camera} are used in detecting DE,
	with $r_\text{statistics}$  estimated from sample correlation to adjust for inter-gene correlation.
	Our study shows, however, that for DE genes, $r_\text{statistics}$ may be either overestimated if
	two genes are positively correlated, or underestimated if two genes are negatively correlated. If we
	believe that most genes are positively correlated (if any) and that there are true DE genes, then
	the VIF factor may be overestimated in \cite{wu2012camera}, which may result in conservative test
	for enrichment analysis; the variance of  $r_\text{statistics}$ may also be overestimated in
	\cite{efron2007correlation}, which leads to larger variation in estimating conditional FDP. The
	situation may be more complicated for RNA-Seq data, which are counts in nature and therefore need to
	be modeled by more sophisticated regression tools (e.g. logistic regression, negative binomial
	regression, etc.). 
	
	\textbf{	Acknowledge the study’s limitations \\}
	\textit{One assumption yet to be justified}\\
	In the context of two sample $t$-test, the simulation results agree with our theoretical
	conclusion, assuming that  $0\leq r_s \leq |\rho|$ in (\ref{rhoineq}) is true. Our simulation does
	suggest 
	\begin{equation}\label{rhosrho}
		r_s = \rho^2, 
	\end{equation}
	as shown in figure (\ref{invsd}). If (\ref{rhosrho}) can be justified theoretically, it is possible
	to approximate the true value of $\rho(T_1, T_2)$, which will correct the bias of estimating
	$r_\text{statistics}$ by $r_\text{sample}$. Another remaining challenge is to assess the
	relationship of $r_\text{statistics}$  and $\rho$ for non-normal distributions, or for other
	hypothesis testing under different regression models (e.g., negative binomial regression).  
	
	% \textbf{Make suggestions for further research\\ }
	
	
	\newpage
%	\section{Appendix}

	
	\bibliographystyle{apalike}
	\bibliography{mybib}
	
\end{document}





