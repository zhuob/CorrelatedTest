\documentclass[12pt, a4paper]{article}
\usepackage{graphicx}
\usepackage{amsmath,bm}
\usepackage[square,numbers]{natbib}
%\usepackage[square, numbers, authoryear]{natbib}
\usepackage[utf8]{inputenc}    
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{multirow}
\title{Estimating test statistics correlation from sample correlation}
\date{} % Today's date or a custom date
\usepackage{epsfig}
\usepackage{subcaption}




\newtheorem{theorem}{Theorem}       % theorem environment
\newtheorem{definition}{Definition}     % definition environment
\newtheorem{proof}{Proof}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{corollary}

\newcommand{\cov}{\text{Cov}}
\newcommand{\cor}{\text{Corr}}
\newcommand{\var}{\text{Var}}
\newcommand{\samplecor}{sample correlation}
\newcommand{\popucor}{population correlation}

\begin{document}
	\maketitle
	\section{Introduction}
	\textbf{What's the consequence if the correlation between statistics cannot be represented sample correlation?}
	\begin{enumerate}
		\item Methods relating FDR control in terms of type I error seems to be OK?? Because under the null, test statistics correlation are (almost) the same as sample correlation.
		\item What about power in terms of FDR control?
		\item Competitive gene set test would definitely be affected, in terms of both type I error and power.  
		\item it seems, according to Efron's 2007 paper, that conditional FDR will also be affected. 
	\end{enumerate}
	
	\textbf{What problem do we address in this paper?}\\

	
	\textbf{Introduction}\\
	In gene expression experiments, inter-gene correlations are commonly observed in expression data \citep{efron2012large, qiu2005effects,barry2008statistical, efron2007correlation,storey2003positive, huang2013gene, wu2012camera, gatti2010heading}. The key task of expression analysis is to detect differentially expressed (DE) genes. One common feature of such DE detection is that a summary statistic is calculated for each gene to measure the magnitude of DE. The test statistics are often of familiar form, for example, two-sample comparison or experimental design based regression. However, those test statistics are likely to be correlated, since their corresponding expression levels are correlated. This paper concerns the relation between test statistics correlations and the corresponding expression level correlations.
	
	\textbf{Why would people care about correlation between genes?}\\
	 The stochastic dependence of test statistics has brought methodological issues, in terms of accessing both individual genes and gene sets. The interest in examining individual genes is to find DE genes among tens of thousands of candidates. Multiple hypothesis testing procedures, such as \textit{false discovery rate} (FDR, \citealt{benjamini1995controlling}) and \textit{$q$-value} \citep{storey2003positive}, are therefore needed. In many cases, such techniques work only when test statistics are independent \citep{benjamini1995controlling} or have positive regression dependency \citep{benjamini2001control}. The goal of evaluating gene sets is to find molecular pathways or gene networks that are related to the experimental condition or factors of interest. Testing a gene set is usually done by pooling the test statistics of its member genes, and may or may not involve genes not in the test set \citep{goeman2007analyzing}. In all situations, the correlation between test statistics is a nuisance aspect, which, if not addressed appropriately, will undermine the applicability of the corresponding approaches (REF). For
	 example, \citet{efron2007correlation} showed in a simulation study that for a nominal FDR of 0.1, the
	 actual FDR can easily vary by a factor of 10 when correlation between test statistics exists. 
	 
	 \textbf{What are existing ways of dealing with inter-gene correlations?}\\
	 A number of attempts have been made to deal with issues of inter-gene correlation when testing either individual genes or gene sets. One approach is to derive certain summary statistic from correlation among test statistics and then use it in the hypothesis testing procedure. (Do I need more examples here) For testing individual genes, \citet{efron2007correlation} calculates the \textit{false discovery proportion} (FDP) conditioning on some dispersion variate which is estimated from correlation among transformed test statistics.  For testing gene sets, \citet{wu2012camera} estimate a \textit{variance inflation factor} (VIF) associated with inter-gene correlation and incorporate it into their parametric/rank-based testing procedures. The same VIF is also used by \citet{yaari2013quantitative} to account for correlation in their distribution-based gene set testing procedure. Another approach is to permute the labels of biological samples. Sample permutation generates the null distribution of test statistic for each gene.
	 This type of permutation preserves underlying correlation structure between genes, and thus protect the test against such correlations (REF, FDR related and gene set test related). However, sample permutation method has an extra assumption, which states that the test statistics always follow the distribution they have under complete null  that no gene is DE \cite{efron2012large1}. In other words, this assumption expects that the distribution of test statistics under the null is not affected by the presence of non-null cases. The \textit{gene set enrichment analysis} (GSEA) procedure \citep{subramanian2005gene} falls into this category.
	 
	\textbf{Key question: Are  expression level correlations the same as test statistics correlation?}\\
	The first approach requires that the correlations between test statistics are known or at least can be estimated from the data. Without replicating the experiment, however, there's no way to obtain the correlation structure of test statistics because only a single test statistic is available for each gene. In the case of one-sided test (e.g., two sample $t$-test), one possible choice is to use sample correlations (after gene treatment effects nullified) to represent correlations among test statistics, as is done by \citet{barry2008statistical, efron2007correlation, wu2012camera}. 
	In all of the three works, it is shown by simulation only the
	equivalence (in terms of either distribution or numerical summarization) of sample correlation coefficient and test statistics correlation coefficient. 
	\citet{efron2007correlation} estimates the distribution of $z$-value (transformed from corresponding two sample $t$-test statistics) correlation by sample correlation. \citet{barry2008statistical} show by Monte Carlo simulation of gene expression data that a nearly linear relationship holds between test statistic correlation and sample correlation for several types of test statistic.
	%The validity of such representation is then demonstrated by small simulations.
	% \citet{efron2007correlation} used $z$-values converted from corresponding two sample $t$-test statistics by 
%	\begin{equation}\label{t2z}
%		z  = \Phi^{-1}(G_0(t))
%	\end{equation}
%	where $\Phi$ is the cumulative distribution function (CDF) for $N(0, 1)$ and $G_0$ is a putative
%	null CDF for $t$-values. The correlation of $z$-values were approximated by residual sample
%	correlation, since it was demonstrated via simulation that the distribution of residual sample
%	correlation applies to that of $z$-value correlation. \citet{barry2008statistical} showed by Monte
%	Carlo simulation of gene expression data that a nearly linear relationship holds between test
%	statistic correlation and residual sample correlation under several standard experimental design.
%	The sample correlation was then used to account for test statistics correlation in their bootstrap
%	method of enrichment test. \citet{wu2012camera} assumed that genewise $t$-test statistics correlation
%	is the same as residual sample correlation, and calculated the mean of all pairwise sample
%	correlation to estimate the VIF, a vital factor in adjusting for inter-gene correlation in their
%	enrichment test procedures. 
	 It has, to the best of our knowledge, not yet been fully explored in
	the context of two group comparison.
	
	
		\textbf{What did we find}\\
		In this work, we investigated the effect of testing procedures on inter-gene correlation structure
		regarding two group comparison. Theoretically, we proved that for two sample $z$-test, there is a
		perfect positive correlation between sample correlation coefficient $r_{\text{sample}}$ and test
		statistics correlation $r_{\text{statistic}}$. For two sample $t$-test, the equivalence does not
		hold in general for $r_{\text{statistic}}$ and $r_{\text{sample}}$, unless all the test are true
		null (no DE).  We demonstrated by simulation that under the null, such equivalence also holds for
		two group comparison of Poisson regression. 
		
		
	
	\textbf{Relevant but different work}\\
	A relevant research was done by \citet{qiu2005effects}, in which they studied the effect of different
	normalization procedures on the inter-gene correlation structure for microarray data. They randomly
	assigned 330 arrays into 15 pairs, each containing 22 arrays within each array 12558 genes. Then 15
	$t$-statistics were calculated for each gene to mimic 15 two-sample comparisons under null
	hypothesis of no DE. They compared the histogram of $t$-statistics correlation for different
	normalization algorithms, and concluded that the normalization procedures are unable to completely
	remove the correlation between the test statistics. % In this work, our interest is in evaluating
	%the effect of several testing procedures on gene expression correlation. 
	

	
	
	
	
	
	\section{General setup}
	
	\subsection{define what do we mean by correlation}
	\textit{Correlation} is a statistical quantity used to assess a possible linear relationship between two random variables or two sets of data sets. The degree of correlation is measured by \textit{correlation coefficient}, a scaler taking values on the interval $[-1, 1]$. Correlation coefficient of +1 (-1) indicates perfect positive (negative dependence), while correlation coefficient of 0 implies no linear relationship between two random variables. Larger correlation coefficient (in absolute value) corresponds to stronger linear correlation. 
	There are many ways to look at the correlation coefficient , many of which are special cases of Pearson's correlation coefficient \citep{lee1988thirteen}. For example, the \textit{Kendall tau rank correlation coefficient} is computed as Pearson's correlation coefficient between the ranked variables.
	
	Let $(X, Y)$ be a random vector, and $(x_j, y_j)$ its $j$th observation. The most familiar measure of dependence between two quantities is the \textit{Pearson's correlation coefficient}.  Following the notation of \citet{lee1988thirteen}, We will restrict our interest to two types of Pearson's correlation coefficient: 1) standardized covariance, which we refer to as \textit{\popucor}
	\begin{equation}\label{eq:popucor}
		\rho =\dfrac{\cov(X, Y)}{\sqrt{\var(X)\var(Y)}} = \dfrac{E[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X\sigma_Y}, 
	\end{equation} 	where $\mu_X$ and $\mu_Y$ are the expected values and $\sigma_X<\infty$ and $\sigma_Y<\infty$ are the population standard errors, and 2) a function of raw scores and means, which we refer to as \textit{\samplecor}
	\begin{equation}\label{eq:samplecor}
	r  =  \dfrac{\sum_j (x_j -\bar{x})(y_j - \bar{y})}{\sqrt{\sum_{j}(x_j - \bar{x})^2\sum_i(y_j - \bar{y})^2}}, 
	\end{equation}
	where $(\bar{x}, \bar{y})$ is the vector of arithmetic mean of the observations.
	Throughout this paper, we will discuss the correlation between $X$ and $Y$ under bivariate settings. 


	In a two group comparison experiment (e.g., gene expression of treatment and control), let $(X_j, Y_j)$ be the expression level for a pair of genes in sample $j$ where $j=1, \ldots, n_1$ if it's from the treatment group and $j=n_1 + 1, \ldots, n_1 + n_2$ if it's from the control group. We assume that the covariance for different samples are the same, but the mean level might differ between the two groups, that is,  
	\begin{equation}\label{eq:covstruct}
		\cov\left(\begin{array}{c}
			X_j\\
			Y_j\\	
		\end{array} \right)	
		= \left(
		\begin{array}{cc}
		\sigma_X^2 &\rho \sigma_X\sigma_Y \\
		\rho \sigma_X \sigma_Y & 	\sigma_Y^2 \\
		\end{array} 
		\right)
		\stackrel{\text{def}}{=} \bm \Sigma 
	\end{equation}
	and 
	\begin{equation}\label{eq:meanstruct}
	\begin{aligned}
	E  \left(\begin{array}{c}
	X_j\\
	Y_j\\	
	\end{array} \right) 
	& = 	\left(\begin{array}{c}
	\mu_X\\
	\mu_Y\\
	\end{array} \right)\stackrel{\text{def}}{=} \bm \mu,  \text{~~ for $j = 1, \ldots, n_1$} \\
	E  \left(\begin{array}{c}
	X_j\\
	Y_j\\	
	\end{array} \right) 
	&= 	\left(\begin{array}{c}
	\mu_X + \Delta_X\\
	\mu_Y + \Delta_Y\\
	\end{array} \right)\stackrel{\text{def}}{=} \bm \mu + \bm \Delta,  \text{~~ for $j = n_1 + 1, \ldots, n_1+ n_2$}.	
	\end{aligned}	
	\end{equation}
	Here $\rho$ is the population correlation defined by equation (\ref{eq:popucor}), and $\bm \Delta= (\Delta_X, \Delta_Y)$ is vector of treatment effect for the pair of genes.
	In addition, We assume independence across samples (Note that independence implies 0 correlation, but not vise versa), 
	\begin{equation}\label{eq:indepsamples}
	\cov(X_{j_1}, X_{j_2}) = \cov(Y_{j_1}, Y_{j_2}) = 0 \text{~~~for $j_1\neq j_2$}
	\end{equation}
	 In the context of gene expression study, the goal is to detect differential expression (DE)---the mean difference between treatment and control group, which can be statistically formulated as 
	 \begin{equation}\label{eq:hypotheses}
	 H_{0i}:  \Delta_i = 0 \textit{     Verses   }  H_{1i}: \Delta_i\neq 0 ,  ~~ i = X, Y.
	 \end{equation}
	This hypothesis testing procedure usually results in a ``$t$-test similar" test statistic for each gene. Without a loss of generality, we express the test statistics as follows
	\begin{equation}\label{eq:teststat}
	T_X = \dfrac{\bm a^T\bm X}{S_X},  ~~~ T_Y = \dfrac{\bm a^T \bm Y}{S_Y}
	\end{equation}  
	where $\bm a$ is a vector of length $n_1 + n_2$ denoting coefficient for comparison(e.g., $1$ for treatment and $-1$ for control), $\bm X = (X_1, \ldots, X_{n_1}, X_{n_1 +1}, \ldots, X_{n_1 + n_2})$, and  $S_X$ and $S_Y$ are the standard error for $\bm a^T\bm X$ and $\bm a^T\bm Y$ respectively. Depending on the type of test, the standard error $S$ may take different forms. In the case of two sample $t$-test with unequal variance, $\bm a^T\bm X = \bar{X}_1 - \bar{X}_2$ and $S_X = \sqrt{S_{X,1}^2/n_1 + S_{X,2}^2/n_2}$, where $ S_{X,1}^2$ and $S_{X,2}^2$ are the sample variances for the treatment and the control groups,
	\begin{equation}\label{eq:samplevariance}
	S_{X, 1} =  \frac{\sum_{j=1}^{n_1}(X_j - \bar{X}_1)^2}{n_1 -1}, ~~S_{X, 2} =  \frac{\sum_{j=n_1 +1}^{n_1 + n_2}(X_j - \bar{X}_2)^2}{n_2 -1}
	\end{equation}
	
	
	Our main goal is to explore the relationship between population correlation (equation (\ref{eq:popucor})) for the test statistics 
	\begin{equation}
	\rho_T = \cor({T_X, T_Y}),
	\end{equation}  
	and that for their corresponding expression level 
	\begin{equation}
	\rho = \cor(X, Y). 
	\end{equation}
	We will examine ???HOW MANY??? different test statistics having the form of equation (\ref{eq:teststat}).%, for a given sample size of $n_1$ for treatment group and $n_2$ for control group.
	
	
	\section{Results}\label{section:tcorresults}
	
	In this section we present the exact expression of statistics correlation coefficient for two
	sample $t$-test. In the first part, we conclude theoretically that test statistics correlation and
	sample correlation are perfect positive dependent for two sample $z$-test, but that is not always
	true for two sample $t$-test. In the second part, we simulate four different cases where test
	statistics correlation $r_{\text{statistics}}$ may be very different from true correlation $\rho$ or
	sample correlation $r_{\text{sample}}$. 
	\subsection{Theory}
%	\subsubsection{$S$ is a constant}
	\begin{theorem}\label{thm:lineartransformation} 
		For any given sample size $(n_1, n_2)$ and non zero $\bm a$, $\rho_T=\rho$ if $S_X$ and $S_Y$ are constant with respect to $\bm X, \bm Y$. 
	\end{theorem}
	\text{Proof:} Since samples are independent, we have 
	\begin{equation}
	\begin{aligned}
		\cov(\bm a^T\bm X, \bm a^T\bm Y) &= \bm a^T \cov(\bm X, \bm Y)\bm a  = \rho\sigma_X\sigma_Y\bm a^T\bm a, \\
		\var(\bm a^T\bm X)& = \bm a^T\var(\bm X)\bm a   = \sigma_X^2\bm a^T\bm a, \\
		\var(\bm a^T\bm Y)& = \bm a^T\var(\bm Y)\bm a   = \sigma_Y^2\bm a^T\bm a 
	\end{aligned}
	\end{equation}
	It follows by equation (\ref{eq:popucor}) that
	\begin{equation}
	\rho_T = \dfrac{\cov(T_X, T_Y)}{\sqrt{\var(T_X)\var(T_Y)}} = \dfrac{\cov(\bm a^T\bm X, \bm a^T\bm Y)/S_XS_Y}{\sqrt{\var(\bm a^T\bm X)\var(\bm a^T\bm Y)}/S_XS_Y} = \rho
	\end{equation}
	\textbf{Note:} Theorem \ref{thm:lineartransformation} states that test statistics correlation and expression level correlation are equal under linear transformation of $\bm X$ and $\bm Y$. Let $\bm a  = (\underbrace{\frac{1}{n_1}, \ldots, \frac{1}{n_1}}_{n_1}, \underbrace{-\frac{1}{n_2}, \ldots, -\frac{1}{n_2}}_{n_2})$, and if we set $S_X=1$, then $T_X$ corresponds to mean difference between treatment and control group; instead, if $S_X = \sigma_X\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$, then $T_X$ corresponds to the statistic for two sample $z$-test. Therefore, $\rho_T=\rho$ if we use mean difference or $z$-statistic as test statistic.
	
	
	\begin{theorem}\label{thm:tstat}
		Let $(X_i, Y_i), i = 1, \ldots, n_1 + n_2$ follow a bivariate normal distribution with mean specified in equation (\ref{eq:meanstruct}) and covariance in (\ref{eq:covstruct}). If $T_X$ and $T_Y$ are statistics for equal-variance two-sample $t$-test, that is, 
		\begin{equation}\label{eq:tstatform}
			\begin{aligned}
				&\bm a^T\bm X  = \bar{X}_1 - \bar{X}_2 \stackrel{def}{= }U_X, ~~\bm a^T\bm Y = \bar{Y}_1 -\bar{Y}_2 \stackrel{def}{=}U_Y \\
				&S_i^2 = \frac{(n_1-1)S_{i, 1}^2 + (n_2 -1)S_{i,2}^2}{n_1 + n_2 -2}, ~~i = X, Y.
			\end{aligned}
		\end{equation}
		then 
		\begin{equation}\label{eq:ttestcor}
	\begin{aligned}
	 \cor(T_X, T_Y) =   
	 \frac{\frac{\Delta_X\Delta_Y}{\sigma_X\sigma_Y}C \rho_{s}+ \rho B
		+ \rho_{s}\rho(A-B)}{\sqrt{\left[ \frac{\Delta_X^2}{\sigma_X^2}C + A\right]\left[\frac{\Delta_Y^2}{\sigma_X^2}C +   A\right]}}
	\end{aligned}
		\end{equation}
	where 
			 \begin{equation}\label{eq:AandB}
			 	 \begin{aligned}
			 	 A & = \frac{n_1 + n_2-2}{n_1 + n_2-4}, ~~B =
			 	 \frac{(\frac{n_1 + n_2 -2}{2})\Gamma^2(\frac{n_1 + n_2 -4}{2} + \frac{1}{2})}{\Gamma^2(\frac{n_1+ n_2 -2}{2})}, \\
			 	 \rho_s & = \cor(\frac{1}{S_X}, \frac{1}{S_Y}), ~~ 
			 	 C = \frac{(n_1 + n_2)(A-B)}{(2 + n_1n_2^{-1} + n_1n_2^{-1})}.
			 	 \end{aligned}
			 \end{equation}	 
	\end{theorem}
	The proof of Theorem \ref{thm:tstat} is presented in Section \ref{section:testcormethod}. Next we give the limit of $\cor(T_X, T_Y)$.
	\begin{theorem}\label{thm:rholimit}
	  	If there exists positive constant $M_1$ and $M_2$, such that $M_1 \leq n_1n_2^{-1}\leq M_2$, then
	   \begin{equation}\label{limitT}
	   \rho_T=\lim\limits_{n_1 + n_2 \rightarrow \infty} \cor(T_X, T_Y) = \frac{\rho +
	   	\beta\frac{\Delta_X\Delta_Y}{\sigma_X\sigma_Y}\rho_{s}}{\sqrt{  \left[ 1 +\beta\frac{\Delta_X^2}{\sigma_X^2}\right]\left[ 1 + \beta\frac{\Delta_Y^2}{\sigma_Y^2}\right]}}
	   \end{equation}
	   where $\rho_{s}$ is defined in equation (\ref{eq:AandB}) and $\beta = \lim\limits_{n_1 + n_2 \rightarrow \infty}C = (4 + 2n_1^{-1}n_2 + 2n_1n_2^{-1})$.
	\end{theorem}
	Theorem \ref{thm:rholimit} says that as long as $n_1$ and $n_2$ grow proportionally, the limit of $\cor(T_X, T_Y)$ is a function of population covariance $\bm \Sigma$, the true mean difference $\bm \Delta$ and the ratio $n_1/n_2$. We have the following observations:
	\begin{enumerate}
		\item If both test are true null (i.e., $\bm \Delta = \bm 0$), then $\rho_T = \rho$.
		\item If one test is true null, then $\rho_T$ is proportional to and smaller in absolute value than $\rho$. For example, when $\Delta_X=0$, $\rho_T = \rho /\sqrt{1 + \beta \frac{\Delta_Y^2}{\sigma_Y^2}}$. 
		\item If both tests are true alternative (i.e., $\bm \Delta \neq \bm 0$), then $\rho_T\neq \rho$ in general.
	\end{enumerate}
	
	However, it should be noted that if $n_1/n_2 \rightarrow 0$ or $\infty$, then $\beta = 0$ and we have $\rho_T = \rho$. That is, when sample size of one group is dominant, $\cor(T_X, T_Y)$ will converge to $\rho$ regardless of whether the tests are under the null or not. 
	
%	 When $\bm \Delta = \bm 0$ then $\rho_T = \rho$; but when $\bm \Delta \neq \bm 0$, then $\rho_T \neq \rho$ in general. % In the next section, we will discuss about it in further detail. 
%	\begin{corollary} 
%	If $\bm \Delta  = (\Delta_X, \Delta_Y)= \bm 0$, and there exists a positive number $M$, such that  $n_1n_2^{-1}\leq M$ and $n_1n_2^{-1}\leq M$,  then $\cor({T_X, T_Y})\rightarrow \rho$ as $n_1 + n_2 \rightarrow \infty$. 
%	\end{corollary}
%	Proof:  If null is true for both test or $\bm \Delta  = 0$, then equation (\ref{eq:ttestcor}) reduces to
%	\begin{align}\label{CalculateTCor}
%		\cor(T_X, T_Y) = \left[\rho_s \cdot 1 + (1-\rho_s)\frac{B}{A}\right]\rho
%	\end{align}
%	The term in the square bracket is a weighted average of 1 and $\frac{B}{A}$, with the latter
%	converging to 1 as $n_1 + n_2$ grows to infinity. Therefore $\lim\limits_{n_1 + n_2\rightarrow\infty} \cor({T_X, T_Y}) = \rho$.
%	\begin{corollary} 
%		If $\bm \Delta = (\Delta_X, \Delta_Y)\neq \bm 0$, then  $\cor({T_X,
%			T_Y})$ does not converge to $\rho$ in general.
%	\end{corollary} 
%	The result immediately follows from lemma  (\ref{lemmaLimit}) in appendix.

	
	
	Depending on the underlying value of $\bm\Delta$ (DE or not DE, up-regulated or down-regulated if DE) and covariance $\bm \Sigma$,
	$\rho_T$ might be far from $\rho$ in different ways. Next we will discuss the relationship between $\rho_T$ and $\rho$ under several scenarios.
	
	We show via simulation (see Figure \ref{invsd}) that for $\rho$ growing from -1 to 1, $\rho_s$
	in equation (\ref{eq:AandB}) has a ``U" shape whose minimum is located near $\rho=0$, and  
	\begin{equation}\label{rhoineq}
		0 \leq r_s \leq |\rho| \text{~~ ONLY BASED ON SIMULATION}
	\end{equation}
	which is useful in comparing  $\rho_T$  and $\rho$. 
	For $\rho <0 $
	\begin{enumerate}
		\item if $\Delta_X\Delta_Y >0$, then gene $X$ and gene $Y$ are DE in the same direction (both 
		up-regulated or both down-regulated), then 
		\begin{equation}
			\rho_T= \frac{\rho + \beta\frac{\Delta_X\Delta_Y}{\sigma_X\sigma_Y}\rho_s}{\sqrt{ 
					\left[ 1 + \beta\frac{\Delta_X^2}{\sigma_X^2}\right]\left[ 1 + \beta\frac{\Delta_Y^2}{\sigma_Y^2}\right]}} 
			>\frac{\rho }{\sqrt{  \left[ 1 + \beta\frac{\Delta_X^2}{\sigma_X^2}\right]\left[ 1 +
					\beta\frac{\Delta_Y^2}{\sigma_Y^2}\right]}} >\rho
		\end{equation}
		\item  if $\Delta_X\Delta_Y <0$, then gene $X$ and gene $Y$ are DE in different directions (one
		up-regulated and the other down-regulated), then by $\rho_s < -\rho$, 
		\[\rho(T_X, T_Y)= \frac{\rho + \beta\frac{\Delta_X\Delta_Y}{\sigma_X\sigma_Y}\rho_{\text{s}}}{\sqrt{ 
				\left[ 1 + \beta\frac{\Delta_X^2}{\sigma_X^2}\right]\left[ 1 + \beta\frac{\Delta_Y^2}{\sigma_Y^2}\right]}} 
		>\rho\frac{ 1-\beta\frac{\Delta_1\Delta_2}{\sigma_1\sigma_2}}{\sqrt{  \left[ 1 +
				\beta\frac{\Delta_X^2}{\sigma_X^2}\right]\left[ 1 + \beta\frac{\Delta_Y^2}{\sigma_Y^2}\right]}} >\rho\]
		\item if  $\Delta_X\Delta_Y =0$, then one is DE but the other is not. Suppose gene $X$ is not DE,
		then
		\[\rho(T_X, T_Y)= \frac{\rho}{\sqrt{  \left[ 1 + \beta\frac{\Delta_X^2}{\sigma_X^2}\right]}} >\rho \]
		
	\end{enumerate}
	Therefore in any case, $\rho_T \geq \rho$ when $\rho <0$. Similarly it can be shown that for
	$\rho >0$, $\rho_T \leq \rho $. In simple words,  $T_X$ and $T_Y$ are "less" correlated than their corresponding expression values are. 
	\begin{figure}[!h]
		\centering
		\caption{$\cor(S^{-1}_X, S^{-1}_Y)$ against $\rho$}
		\includegraphics[scale=0.25]{./Figures/invsd.png}
		\label{invsd}
	\end{figure}
	
	
	
	\subsection{Simulation}
	The simulations are performed under two different testing procedures. The first is two sample
	$t$-test, where we evaluate the true correlation and statistics correlation.  In the second setting,
	we simulate correlated Poisson data to mimic RNA-Seq counts, and evaluate the relationship between
	the two for score test of Poisson regression. \\
	
	For the normal case, we let 
	\begin{equation}
		\begin{aligned}
			\bm X_i &=\left( \begin{array}{c}
				X_{1i}\\
				X_{2i}\\
			\end{array}\right)
			\sim N\left[
			\left(\begin{array}{c}
				10\\
				-10\\
			\end{array} \right), 
			\left(
			\begin{array}{cc}
				0.1 &\rho \sqrt{0.1\cdot 0.3} \\
				\rho \sqrt{0.1\cdot 0.3} & 	0.3 \\
			\end{array}
			\right)
			\right] \\
			\bm Y_j &= \left( \begin{array}{c}
				Y_{1j}\\
				Y_{2j}\\
			\end{array}\right)
			\sim N\left[
			\left(\begin{array}{c}
				10+ \Delta_1\\
				-10 + \Delta_2\\
			\end{array} \right), 
			\left(
			\begin{array}{cc}
				0.1 &\rho \sqrt{0.1\cdot 0.3} \\
				\rho \sqrt{0.1\cdot 0.3} & 	0.3 \\
			\end{array}
			\right)
			\right] 
		\end{aligned}
	\end{equation}
	with $\rho$ growing continuously from -0.99 to 0.99 by 0.01.  The sample size $n$ is set to be 1000
	($j =1, \ldots, 500$ for each group). For each given $\rho$, we generate $50,000$ samples for
	control group and another $50,000$ samples for the treatment group. The $50,000$ samples within each
	group are then randomly split into 100 blocks of size 500. Next, a pair is formed by taking one
	block (500 samples) from treatment and one block from control,  mimicking one experiment for two
	group comparison. Therefore, 100 pairs are obtained to represent 100 replicates of the same
	experiment, from which 100 test statistics are computed for each gene. 
	
	The sample correlation $r_{\text{sample}}$ is calculated by (\ref{rhosample}). The correlations
	between $t$-test statistics are calculated by the sample correlation of ($\bm T_1, \bm T_2, \ldots,
	\bm T_{100}$). Specifically, we compared $r_\text{statistics}$, $r_{\text{sample}}$ and $\rho$ for
	the following four cases:
	\begin{enumerate}
		\item[a)] no DE genes;
		\item[b)]  DE in opposite directions;
		\item[c)] DE in the same direction; 
		\item[d)] gene 1 DE and gene 2 null.
	\end{enumerate}
	Figure (\ref{figureT}) plots $r_\text{statistics}$ and $r_{\text{sample}}$ against $\rho$. While the
	equivalence between those three holds when neither gene is DE [case a)], it fails as long as DE
	exists. $r_\text{statistics}$ is almost always negative, if genes are DE in different direction
	[case b)], and almost always positive if genes are DE in the same direction [case c)]. When only one
	gene is DE,  $r_\text{statistics}$ is positively proportional to $\rho$ [case d)]. Note that in all
	cases, $|r_{\text{statistics}}| \leq |\rho|$, in other words, the test statistics tend to be "less
	correlated" than the samples are.
	
	\begin{figure}[!ht]
		\centering
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{./Figures/NonDET.eps}
			\captionsetup{justification=centering}
			\caption{$\bm \Delta= (0, 0)$}
			% \label{fig:fitsa}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{./Figures/DET.eps}
			\captionsetup{justification=centering}
			\caption{$\bm \Delta= (2, -2)$}
			% \label{fig:fitsb}
		\end{subfigure}
		\newline
		\newline
		\centering
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{./Figures/DETsame.eps}
			\captionsetup{justification=centering}
			\caption{$\bm \Delta= (2, 2)$}
			% \label{fig:fitsa}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{./Figures/DETone.eps}
			\captionsetup{justification=centering}
			\caption{$\bm \Delta= (-2, 0)$ }
			% \label{fig:fitsb}
		\end{subfigure}
		\caption{Under $t$-test, the relationship between $r_{\text{sample}}$ (solid dots), $
			r_\text{statistics}$ (triangles) and $r_{\text{true}}$ (horizontal axis), for the case (a): gene 1
			and gene 2 are not DE; (b): both genes are DE, but in different direction; (c): both genes are DE,
			in the same direction; (d): gene 1 is DE, but gene 2 is not. $\bm \Delta$ is the magnitude of DE.}
		\label{figureT}
	\end{figure}
	
	
	
	Correlated Poisson data was simulated according to \cite{madsen2013simulating}. Briefly, a 2-vector
	standard normal $\bm Z$ is first generated with correlation matrix $\bm \Sigma_Z$, and $Z_i$'s are
	converted to $U_i= \Phi(Z_i)$ where $\Phi$ is standard normal CDF. $U_i$'s, uniform on (0, 1), are
	then transformed to $Y_i \equiv F_i^{-1}(U_i)$ with 
	\begin{align}\label{inverseSim}
		F_i^{-1}(u) = \inf \{y: F_i(y)\geq u\}
	\end{align}
	The element in $\bm \Sigma_Z$ are chosen such that the desired Pearson correlation can be achieved.
	Technical details are available in \cite{madsen2013simulating} and thus not discussed here. 
	
	The two group comparison under Poisson regression are simulated as follows: for control group
	$X_{1i} \sim \text{Pois}(20)$ and $X_{2i}\sim $ Pois(50)   with $\cov(X_{1i}, X_{2i}) = \rho$;
	for treatment group, a shift $\bm \Delta$ is added to the mean vectors, in other words, $Y_{1i} \sim
	$ Pois($20 + \Delta_1$) and $Y_{2i} \sim $ Pois($50 + \Delta_2$) with $\cov(Y_{1j}, Y_{2j}) =
	\rho$. The test statistics are calculated from score test (derivation is available in appendix),
	\begin{align}\label{Poisscoretest}
		U = \frac{\sqrt{\frac{n}{2}}(\bar{y}_1- \bar{y}_2)}{\sqrt{\bar{y}_1 + \bar{y}_2}}.
	\end{align}
	
	Unlike the normal distribution whose shape is determined by both the mean and variance parameters,
	the shape of a Poisson distribution is totally determined by its mean parameter. For a score test
	statistic such as (\ref{Poisscoretest}), the denominator and the numerator are no longer
	independent. Subsequently, the derivation of test statistics correlation for $t$-test is invalid for
	Poisson regression. We will only demonstrate via simulation the relationship between 
	$r_\text{statistics}$, $r_{\text{sample}}$ and $\rho$.
	
	Figure (\ref{figurePois}) presents the simulation under scenarios a)-d). The equivalence of
	$r_\text{statistics}$, $r_{\text{sample}}$ and $\rho$ still holds in general when neither gene is
	DE.  
	
	
	\begin{figure}[h!]
		\centering
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{./Figures/pois1_1.png}
			\captionsetup{justification=centering}
			\caption{$\bm \Delta= (0, 0)$}
			%  \label{fig:fitsa}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{./Figures/pois2_1.png}
			\captionsetup{justification=centering}
			\caption{$\bm \Delta= (40, -30)$}
			%  \label{fig:fitsb}
		\end{subfigure}
		\newline
		\newline
		\centering
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{./Figures/pois3_1.png}
			\captionsetup{justification=centering}
			\caption{$\bm \Delta= (40, 70)$}
			%  \label{fig:fitsa}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{./Figures/pois4_1.png}
			\captionsetup{justification=centering}
			\caption{$\bm \Delta= (40, 0)$ }
			%  \label{fig:fitsb}
		\end{subfigure}
		\caption{Under score test of Poisson regression, the relationship between $r_{\text{sample}}$
			(solid dots), $ r_\text{statistics}$ (triangles) and $r_{\text{true}}$ (horizontal axis), for the
			case (a): gene 1 and gene 2 are not DE; (b): both genes are DE, but in different direction; (c):
			both genes are DE, in the same direction; (d): gene 1 is DE, but gene 2 is not.}
		\label{figurePois}
	\end{figure}
	
	
	
	
	
	
	\section{Method}\label{section:testcormethod}
	
	\begin{lemma}
		Sample correlation coefficient is a consistent estimator for $\rho$, 
		\[\sqrt{n}(r_{\text{sample}} - \rho ) \stackrel{D}{\rightarrow}N\left(0, (1-\rho^2)^2\right).\]
	\end{lemma}
	The proof of lemma 1 can be found in \citet{fisher1915frequency}. \\
	%$\rho({G_1, G_2})$ and $\frac{{\rho}_X + {\rho}_Y}{2}$ are asymptotically equivalent. \\
	
	To prove Theorem \ref{thm:tstat}, it is useful to note that $\bm U$ is independent of $\bm S$,
	following from Lemmas \ref{lemmabiChisq} and \ref{lemmaIndep}.
	\begin{lemma}\label{lemmabiChisq}
		Let $(X_{j}, Y_{j}), j=1 \ldots,  n$ be independent random variables satisfying equation (\ref{eq:indepsamples}),
		then $\bm W = (W_{X},W_{Y}) =(\frac{(n -1)S_{X}^2}{\sigma_X^2}, \frac{(n-1)S_{Y}^2}{\sigma_Y^2})$ 
		follows a \textbf{bivariate chi square distribution} with density 
		\begin{equation}\label{biChisq}
			\begin{aligned}
				f(w_x, w_y) & = \frac{2^{-n}(w_xw_y)^{(n-3)/2}e^{-\frac{w_x +
							w_y}{2(1-\rho^2)}}}{\sqrt{\pi}\Gamma(\frac{n}{2})(1-\rho^2)^{(n-1)/2}} \times \\
				& \sum_{k=0}^{\infty}[1 +
				(-1)^k]\left(\frac{\rho\sqrt{w_xw_y}}{1-\rho^2}\right)^k\frac{\Gamma(\frac{k+1}{2})}{k!\Gamma(\frac{k
						+ n}{2})}
			\end{aligned}
		\end{equation}
		for $n>3$ and $-1<\rho < 1$.
	\end{lemma}
	For proof of Lemma \ref{lemmabiChisq}, interested readers are referred to \citet{joarder2009moments}.
	It immediately follows from Lemma \ref{lemmabiChisq} that $\bm W_1 = (\frac{(n_1 -1)S_{X, 1}^2}{\sigma_X^2}, \frac{(n_1-1)S_{Y, 1}^2}{\sigma_Y^2})$ follows bivariate chi-square distribution with degree of freedom $n_1-1$, where $S_{X_,1}$ and $S_{X, 2}$ are defined in equation (\ref{eq:samplevariance}). Similarly, $\bm W_2 =(\frac{(n_2 -1)S_{X, 2}^2}{\sigma_X^2}, \frac{(n_2-1)S_{Y, 2}^2}{\sigma_Y^2})$ follows a bivariate chi-square distribution with degree of freedom $n_2-1$.  Note that $\bm W_1$ and $\bm W_2$ are independent since the samples are independent. 
	
	\begin{lemma}\label{lemmaIndep}
		$\bm U =(U_X, U_Y)$ is independent of $\bm S = (S_X ,S_Y)$, where $\bm U$ and $\bm S$ are defined in
		equation (\ref{eq:tstatform}).
	\end{lemma}
	\textbf{Proof}: By Lemma \ref{lemmabiChisq}, the density function of $
	\bm W_1 + \bm W_2$ only involves $\sigma^2_X, \sigma^2_Y, \rho$ and sample size $n_1, n_2$, therefore
	we can denote its density by some function $g(\sigma^2_X, \sigma^2_Y, \rho,
	n_1 + n_2)$. Note that $\bm S^2 = \frac{(\sigma_X^2, ~\sigma^2_Y)}{n_1 +n_2 -2}(\bm W_1 + \bm W_2)^T $
	is a linear transformation of $\bm W_1 + \bm W_2$, so its density also can be expressed in terms of $\sigma^2_1, \sigma^2_2, \rho, n_1, n_2$. Therefore $\bm S = (S_X ,S_Y)$ is an ancillary statistic for $\bm \Delta$. On the other hand, it can
	be shown that $\bm U =(U_X, U_Y)$ is a complete sufficient statistic for $\bm \Delta$. It follows by
	Basu's theorem that $\bm U$ and $\bm S$ are independent. 
	
	
	Lemma \ref{lemmaIndep} implies that  $U_XU_Y$ is also independent of $\frac{1}{S_XS_Y}$, and
	therefore $E(\frac{U_X}{S_X} \cdot\frac{U_Y}{S_Y})$ can be expressed as
	$E(U_XU_Y)E(\frac{1}{S_XS_Y})$. Additionally, if we know $\cor(\frac{1}{S_X}, \frac{1}{S_Y})$,
	then the $t$-test statistics correlation can be accurately represented. 
	
	\textbf{Proof of theorem (\ref{thm:tstat})} \\
	First note that
	\begin{align*}
		\cov(T_X, T_Y) &= E(T_XT_Y) - E(T_X)E(T_Y) \\
		%& = E(c_0\frac{U_1}{S_1} \cdot c_0\frac{U_2}{S_2}) - E(c_0\frac{U_1}{S_1})E( c_0\frac{U_2}{S_2}) \\
		& = \frac{1}{c_0^2} \left[E(U_XU_Y)E(\frac{1}{S_XS_Y}) - E(\frac{U_X}{S_X})E( \frac{U_Y}{S_Y})\right]   
		~~~~~~~~~\text{ (by lemma \ref{lemmaIndep})}
	\end{align*}
	where $c_0 = \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$ and $\var(T_X) = \var(\frac{U_X}{c_0S_X})=
	\frac{1}{c_0^2}\var(\frac{U_X}{S_X})$. 
	Note that 
	\begin{equation}\label{eq:Tcorrelation}
		\begin{aligned}
			\cor(T_X, T_Y) & = \frac{\cov(T_X, T_Y) }{\sqrt{\var(T_X) \var(T_Y) }} \\
			& = \frac{E(U_XU_Y)E(\frac{1}{S_XS_Y}) - E(\frac{U_X}{S_X})E(
				\frac{U_Y}{S_Y})}{\sqrt{\var(\frac{U_X}{S_X})\var(\frac{U_Y}{S_Y})}} 
		\end{aligned}
	\end{equation}
	We need to calculate $E(U_XU_Y)$, $E(\frac{1}{S_XS_Y})$, $ E(\frac{U_i}{S_i})$ and
	$\var(\frac{U_i}{S_i})$ for $i =X, Y$. 
	\begin{enumerate}
		\item Note that $U_i\sim N\left(\Delta_i, \sigma_i^2(\frac{1}{n_1} + \frac{1}{n_2})\right), i=X, Y$. 
		\begin{equation}\label{eq1}
			\begin{aligned}
				E(U_XU_X)&= \cov(U_X, U_Y) + E(U_X)E(U_Y) \\
				&= \rho\sigma_X\sigma_Y\left(\frac{1}{n_1} + \frac{1}{n_2}\right) +
				\Delta_X\Delta_Y
			\end{aligned} 
		\end{equation}
	
		\item Since $\frac{(n_1-1)S_{X}^2}{\sigma_X^2}$ and $\frac{(n_2-1)S_{Y}^2}{\sigma_Y^2}$ are
		independent and follow $\chi^2(n_1-1)$ and $\chi^2(n_2 -1)$ respectively, , we have $W_{X}=\frac{(n_1 + n_2 -2)S_X^2}{\sigma_X^2}\sim
		\chi^2(n_1 + n_2-2)$. It can be shown that 
		\[E(W_{X}^k)= \frac{2^k\Gamma(\frac{n_1 + n_2 -2}{2}+k)}{\Gamma(\frac{n_1 + n_2 -2}{2})}\] 
		Therefore 
		\begin{equation}
		\begin{aligned}
		E\left(\frac{1}{S_X}\right) =
	 \frac{\sqrt{B}}{\sigma_X},		~~~\var\left(\frac{1}{S_X}\right) = \frac{A-B}{\sigma_X^2}
		\end{aligned}
		\end{equation}
			Note that $\rho_s = \cor(\frac{1}{S_X}, \frac{1}{S_Y})$, we have 
			\begin{equation}\label{eq2}
			\begin{aligned}
			E(\frac{1}{S_XS_Y})  &= E(\frac{1}{S_X})E(\frac{1}{S_Y}) + \rho_s
			\sqrt{\var(\frac{1}{S_X})\var(\frac{1}{S_Y})} \\
			& = \frac{B}{\sigma_X\sigma_Y} + \rho_s
			\frac{A-B}{\sigma_X\sigma_Y}
			\end{aligned}
			\end{equation}
	
		\item $U_i\sim N\left(\Delta_i, \sigma_i^2(\frac{1}{n_1} + \frac{1}{n_2})\right)$ and $\frac{(n_1 + n_2 -2)S_i^2}{\sigma_i^2} \sim
		\chi^2(n_1 + n_2-2)$ and by Lemma \ref{lemmaIndep}  $U_i$ and $\frac{(n_1 + n_2 -2)S_i^2}{\sigma_i^2}$ are independent for $i = X, Y$, we have 
		\begin{equation}
			\frac{\frac{U_i-\Delta_i}{\sigma_i\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}}{\frac{(n_1 + n_2-2)S_i^2}{\sigma_i^2}/(n_1 + n_2 -2)}  =
			\frac{U_i-\Delta_i}{S_i\sqrt{\frac{1}{n_1 } + \frac{1}{n_2}}}\sim t(n_1 + n_2-2)
		\end{equation}	
		It follows from 
		\begin{equation}
			E\left(\frac{U_i-\Delta_i}{S_i\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}\right)=0, ~~ \text{Var}\left(\frac{U_i-\Delta_i}{S_i\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}\right) = \frac{n_1 + n_2-2}{n_1 + n_2-4}
		\end{equation}
	 that
	 \begin{equation}\label{eq3}
	\begin{aligned}
	E\left(\frac{U_i}{S_i}\right) &= \frac{\Delta_i}{\sigma_i}\sqrt{B} \\
	\var\left(\frac{U_i}{S_i}\right)&=A\left(\frac{1}{n_1} + \frac{1}{n_2}\right) + \frac{\Delta_i^2}{\sigma_i^2}(A-B)
	\end{aligned}
	 \end{equation}
	\end{enumerate}
	Finally,  the test statistics correlation (\ref{eq:ttestcor}) is obtained by plugging
	equations (\ref{eq1}--\ref{eq3}) into equation (\ref{eq:Tcorrelation}).
	
	Up to now we have obtained an exact expression for $\cor(T_X, T_Y)$, which depends not only on
	the sample size $n_1$ and $n_2$, but also on $\Delta/\sigma$, the relative magnitude of DE. The rest of this section
	discusses asymptotic  property of $\cor(T_X, T_Y)$ for large sample size.
	\begin{lemma}\label{lemmaLimit}
	If there exists a positive number $M$, such that  $n_1n_2^{-1}\leq M$ and $n_1n_2^{-1}\leq M$, then the following results hold:
		\begin{enumerate}
			\item $\lim\limits_{n_1 + n_2\rightarrow \infty} A = 1$.
			\item $\lim\limits_{n_1 + n_2\rightarrow \infty} B = 1$.
			\item $\lim\limits_{n_1 + n_2\rightarrow \infty} C = \beta.$
		\end{enumerate}
	where  $A, B$ and $C$ are defined in equation (\ref{eq:AandB}), and $\beta= (4 + n_1n_2^{-1} + n_1^{-1}n_2)^{-1}$. 
	\end{lemma}
	\textbf{Proof}: Note that 
	\begin{equation}
	B = 
	\begin{cases}
	\frac{(k-1)\Gamma^2(k- \frac{3}{2})}{\Gamma^2(k-1)},& \text{if } n_1 + n_2 = 2k, k\geq 2 \\
	~\\
	 \frac{(k-\frac{1}{2})\Gamma^2(k- 1)}{\Gamma^2(k-\frac{1}{2})},& \text{if } n_1 + n_2 = 2k+1, k\geq 2 \\
	\end{cases}
	\end{equation}
	We will use second order Stirling's formula,
	\begin{align}\label{Stirling1}
		k! \approx \sqrt{2\pi k}\left(\frac{k}{e}\right)^k(1 + \frac{1}{12k})
	\end{align}
	Using Stirling's formula (\ref{Stirling1}) and  $\Gamma(k + \frac{1}{2}) =
	\frac{(2k)!}{4^kk!}\sqrt{\pi}$, it can be shown that 
	\begin{equation}\label{eq:Bapprox}
			B \approx  
		\begin{cases}
			\frac{(k-1)(k-2)(k-2 + \frac{1}{24})^2}{(k-2 + \frac{1}{12})^4},& \text{if } n_1 + n_2 = 2k, k\geq 2 \\
			  ~\\
			\frac{(k-\frac{1}{2})(k - 1 + \frac{1}{12})^4}{(k-1+ \frac{1}{24})^2(k-1)^3},& \text{if } n_1 + n_2 = 2k+1, k\geq 2 \\
		\end{cases}
	\end{equation}
	It can also be shown using equation (\ref{eq:Bapprox}) that
	\begin{equation}\label{eq:AminusB}
		A- B \approx  
		\begin{cases}
		\frac{\frac{1}{4}(k-1)(k-2)^3 + o((k-2)^4)}{(k-2)(k-2 + \frac{1}{12})^4},& \text{if } n_1 + n_2 = 2k, k\geq 2 \\
		~\\
		\frac{\frac{1}{4}(k-1)^3(k-\frac{1}{2})(k-3) + o((k-1)^4)}{(k-\frac{3}{2})(k-1+ \frac{1}{24})^2(k-1)^3},& \text{if } n_1 + n_2 = 2k+1, k\geq 2 \\
		\end{cases}
	\end{equation}
	And the results immediately follow.
	
 
	
	
	
	\section{Conclusion}
	
	\textbf{State the major findings} \\
	This article discusses the relationship between sample correlation coefficients $r_{\text{sample}}$ 
	(after treatment effects removed) and test statistics correlation $r_\text{statistics}$ in a two
	group comparison setting. We proved that under normal distribution assumption, $r_\text{statistics}$
	and $r_\text{sample}$ have a perfect positive correlation for two sample $z$ test. However, for two
	sample $t$-test this correspondence does not hold in general, unless the null in (\ref{hypotheses})
	is true for all the tests considered. The results for two sample $t$-test can be applied to the case
	of two group mean comparison under Poisson regression, as shown by simulation. Consequently, that
	estimating  $r_\text{statistics}$ by $r_{\text{sample}}$ after nullifying treatment effects can not
	be taken for granted.
	
	\textbf{State the practical meaningness of the findings}\\
	In gene expression analysis, cares need to be taken when estimating test statistics correlation from
	sample correlation. For microarray data, two sample $t$ test (\cite{efron2007correlation},
	\cite{barry2008statistical}) or its moderated version \citep{wu2012camera} are used in detecting DE,
	with $r_\text{statistics}$  estimated from sample correlation to adjust for inter-gene correlation.
	Our study shows, however, that for DE genes, $r_\text{statistics}$ may be either overestimated if
	two genes are positively correlated, or underestimated if two genes are negatively correlated. If we
	believe that most genes are positively correlated (if any) and that there are true DE genes, then
	the VIF factor may be overestimated in \cite{wu2012camera}, which may result in conservative test
	for enrichment analysis; the variance of  $r_\text{statistics}$ may also be overestimated in
	\cite{efron2007correlation}, which leads to larger variation in estimating conditional FDP. The
	situation may be more complicated for RNA-Seq data, which are counts in nature and therefore need to
	be modeled by more sophisticated regression tools (e.g. logistic regression, negative binomial
	regression, etc.). 
	
	\textbf{	Acknowledge the studyâ€™s limitations \\}
	\textit{One assumption yet to be justified}\\
	In the context of two sample $t$-test, the simulation results agree with our theoretical
	conclusion, assuming that  $0\leq r_s \leq |\rho|$ in (\ref{rhoineq}) is true. Our simulation does
	suggest 
	\begin{equation}\label{rhosrho}
		r_s = \rho^2, 
	\end{equation}
	as shown in figure (\ref{invsd}). If (\ref{rhosrho}) can be justified theoretically, it is possible
	to approximate the true value of $\rho(T_1, T_2)$, which will correct the bias of estimating
	$r_\text{statistics}$ by $r_\text{sample}$. Another remaining challenge is to assess the
	relationship of $r_\text{statistics}$  and $\rho$ for non-normal distributions, or for other
	hypothesis testing under different regression models (e.g., negative binomial regression).  
	
	% \textbf{Make suggestions for further research\\ }
	
	
	\newpage
	\section{Appendix}
	\begin{appendix}
		
		
		
		\textbf{
			Score test statistics correlation under Poisson regression}\\
		For a gene, let $Y= (Y_1, Y_2, \ldots, Y_n)$ be the gene expression level, and $X= (1, \ldots,
		1, 0, \ldots, 0)$ be the indicator of whether sample is from treatment or control group. A Poisson
		regression model 
		\begin{align*}\label{poisson}
			Y_i\sim Pois(\mu_i) \\
			\log(\mu_i) = \beta_0 +\beta_1 x_i
		\end{align*}
		From the likelihood function 
		\[L = \prod_{i=1}^n \frac{\mu_i^{y_i}}{y_i!}e^{-\mu_i}\]
		we obtain the log-likelihood function
		\begin{align}\label{poissonLikelihood}
			l(\beta_0, \beta_1) & = \log L = \sum_{i=1}^n(y_i\log \mu_i - \log y_i! -\mu_i) \\ \notag
			& = \sum y_i(\beta_0 +\beta_1x_i) - \sum \log y_i! - \sum \exp(\beta_0 + \beta_1x_i) \notag
		\end{align}
		
		
		For testing $H_0:\beta_1=0$, the score test statistics is 
		\[U = [Z(\tilde{\beta})^T I^{-1}(\tilde{\beta}) Z(\tilde{\beta})]^{1/2} \]
		where $\tilde{\beta} = (\hat{\beta}_0, 0)$.   From (\ref{poissonLikelihood}) we have
		\[\frac{\partial l}{\partial \beta_0} = \sum_i y_i - \sum_i \exp(\beta_0) \Rightarrow ~~
		\hat{\beta}_0 = \log (\bar{y})\] 
		Therefore
		\[Z(\tilde{\beta}) = \left[
		\begin{array}{c}
		\sum_i y_i - \sum_i\exp(\beta_0 + \beta_1x_i) \\
		\sum_i y_ix_i -\sum_i \exp(\beta_0 + \beta_1x_i)x_i
		\end{array}
		\right]|_{\beta_1 = 0}
		= \left[
		\begin{array}{c}
		\sum y_i - \exp(\hat{\beta}_0) \\
		\sum y_ix_i - \sum\exp(\hat{\beta}_0)x_i 
		\end{array}
		\right]
		= \left[
		\begin{array}{c}
		0\\
		\sum y_ix_i - \bar{y}\sum x_i 
		\end{array}
		\right]
		\]
		\[I(\tilde{\beta}) = 
		\left[
		\begin{array}{cc}
		\sum \exp (\hat{\beta}_0 + \hat{\beta}_1 x_i)& \sum \exp (\hat{\beta}_0 + \hat{\beta}_1
		x_i)x_i\\
		\sum \exp (\hat{\beta}_0 + \hat{\beta}_1 x_i)x_i & \sum \exp (\hat{\beta}_0 + \hat{\beta}_1
		x_i)x_i^2  \\
		\end{array}\right]
		=\left[\begin{array}{cc}
		\sum y_i  & \bar{y}\sum x_i \\
		\bar{y}\sum x_i  &\bar{y}\sum x_i^2\\
		\end{array}\right]
		\]
		and it follows that 
		\begin{align}\label{scoretest1}
			U =  [Z(\tilde{\beta})^T I^{-1}(\tilde{\beta}) Z(\tilde{\beta})]^{1/2} =\left( \frac{n(\sum
				y_ix_i -\bar{y}\sum x_i)^2}{\bar{y}[n\sum x_i^2 -(\sum x_i)^2]}\right)^{1/2}
		\end{align}
		To simplify the above expression, let's assume the first $n/2$ elements of $\bm X$ are 1,
		therefore we have $\sum x_i =\sum x_i^2=n/2$
		\begin{align}\label{scoretest2}
			U = \sqrt{\frac{n(\sum_{i=1}^{n/2}y_i-\bar{y}\cdot n/2)^2}{\bar{y}[n\cdot n/2 - (n/2)^2]}} =
			\sqrt{\frac{\frac{n}{2} (\bar{y}_1-\bar{y}_2)^2}{\bar{y}_1 + \bar{y}_2}} = \pm
			\frac{\sqrt{\frac{n}{2}}(\bar{y}_1- \bar{y}_2)}{\sqrt{\bar{y}_1 + \bar{y}_2}}
		\end{align}
		where $\bar{y}_1 = \frac{\sum_{i=1}^{n/2}y_i }{n/2}$ and $\bar{y}_2 =  \frac{\sum_{i=n/2
				+1}^{n}y_i}{n/2}$ are just group means. It resembles a $t$ test statistic.
		
		%\textbf{Note:} there is an upper bound for this correlation
		%\[r_{Y_1,Y_2}= \frac{Cov(Y_1, Y_2)}{\sigma_{Y_1}\sigma_{Y_2}} =
		%\frac{\sigma_X^2}{\sigma_{Y_1}\sigma_{Y_2}} \leq \frac{\min (\sigma^2_{Y_1}, \sigma^2_{Y_2})
		%}{\sigma_{Y_1}\sigma_{Y_2}} = \min \left(
		%\frac{\sigma_{Y_1}}{\sigma_{Y_2}},\frac{\sigma_{Y_2}}{\sigma_{Y_1}} \right)\]
		%For each pair of $(\rho, \lambda)$, evaluate the sample correlation and score test statistics
		%correlation UNDER THE NULL. 
		
	
		\newpage
		
	\end{appendix}
	
	\bibliographystyle{apalike}
	\bibliography{mybib}
	
\end{document}































	
	\section{MOVED FROM SETUP}
	
	
	\begin{definition}
		The \textbf{sample correlation} is 
		\begin{equation}\label{rhosample}
		r_{\text{sample}}  = \frac{r_1 + r_2}{2}
		\end{equation}
		where $r_1$  and $r_2$ are sample correlations for expression levels from treatment group and control group, respectively.
		The pooled sample variance for test $H_{0i}$ ($i=1, 2$) is 
		\begin{equation}\label{pooledvariance}
		S_i^2 = \frac{(n-1)S_{X_i}^2 + (n-1)S_{Y_i}^2}{n-1 + n-1} = \frac{S_{X_i}^2 + S_{Y_i}^2}{2}
		\end{equation}
		where $S_{X_i}^2$ and $S_{Y_i}^2$ are sample variances for $X_i$ and $Y_i$ respectively.
	\end{definition} 
	
	
	
	
	Consider two random vectors $\bm X = (X_1, X_2)$ and $\bm Y =  (Y_1, Y_2)$ from normal distribution
	\[ \bm X 
	\sim N\left[
	\left(\begin{array}{c}
	\mu_1\\
	\mu_2\\
	\end{array} \right), 
	\left(
	\begin{array}{cc}
	\sigma_1^2 &\rho \sigma_1\sigma_2 \\
	\rho \sigma_1 \sigma_2 & 	\sigma_2^2 \\
	\end{array}
	\right)
	\right]  \stackrel{\text{def}}{=} N(\bm \mu, \bm \Sigma)
	\] 
	and $\bm Y \sim N(\bm \mu + \bm \Delta, \bm \Sigma)$ where $\bm \Delta = (\Delta_1, \Delta_2)^T$.
	Let $\bm X_1, \ldots, \bm X_n$ and $\bm Y_1, \ldots, \bm Y_n$ be $n$ realizations of each random
	vector. 
	\begin{enumerate}
		\item[A1):] Within a sample, the random variables are correlated, $\cor(X_{i1}, X_{i2}) =
		\cor(Y_{j1}, Y_{j2}) = \rho$.  This is the \textbf{population correlation}.
		\item[A2):] Across samples, the random variables are independent, ($\cor\bm X_{j_1}, \bm Y_{j_2}$)=
		0. 
	\end{enumerate} 
	
	For notational convenience, let $G_1 = (X_{11}, \ldots, X_{1n}, Y_{11}, \ldots, Y_{1n})$ and $G_2 = 
	(X_{21}, \ldots, X_{2n}, Y_{21}, \ldots, Y_{2n})$. We are interested in comparing the mean of $X_1$
	verses $Y_1$, and of $X_2$ versus $Y_2$, statistically formulated as
	\begin{equation}\label{hypotheses1}
	H_{0i}:  \Delta_i = 0 \text{     Verses   }  H_{1i}: \Delta_i\neq 0 ,  ~~ i = 1, 2.
	\end{equation}
	
	
	or $i =X, Y$,  $\bar{X}_{i}\sim N(\mu_i, \sigma_i^2/n)$ and $\bar{Y}_{i}\sim N(\mu_i + \Delta_i,
	\sigma_i^2/n)$. Define
	\begin{equation}\label{expreU}
	U_i = \bar{X}_i - \bar{Y}_i
	\end{equation}
	The two sample $z$-test statistic is
	\begin{equation}\label{zTest}
	Z_i = \frac{\bar{X}_{i}-\bar{Y}_{i} }{\sqrt{\frac{\sigma^2_i}{n} + \frac{\sigma^2_i}{n}}} =
	\frac{U_i}{\sqrt{2\sigma_i^2/n}}\sim N(\Delta_i, 1) ,
	\end{equation}
	and the two sample $t$-test statistic for pooled variance is given by
	\begin{align}\label{t-test}
	T_i = \frac{\bar{X}_i - \bar{Y}_i}{S_i\sqrt{\frac{1}{n}+\frac{1}{n}}} =
	\frac{U_i}{S_i\sqrt{\frac{2}{n}}} \sim t_{2n-2}(\Delta_i). 
	\end{align}
	where $S_i$ the pooled sample standard error defined in (\ref{pooledvariance}) and $\Delta_i$ is the
	non-center parameter. 
	
	\begin{theorem}\label{theoremTstat} 
		Under (A1-A2),  the two sample $t$-test statistics correlation can be expressed by 
		\begin{align}\label{tTestCorrelation}
		\cor(T_1, T_2)  = \frac{r_{\text{s}}\frac{\Delta_1\Delta_2}{\sigma_1\sigma_2}n(A-B) + 2\rho B
			+ 2r_{\text{s}}\rho(A-B)}{\sqrt{\left[ \frac{\Delta_1^2}{\sigma_1^2}n(A-B) + 2
				A\right]\left[\frac{\Delta_2^2}{\sigma_2^2}n(A-B) + 2 A\right]}}
		\end{align} where 
		\begin{align}\label{invScor1}
		r_s = \cor(\frac{1}{S_1}, \frac{1}{S_2}), ~~~~~A = \frac{n-1}{n-2}, ~~~B =
		\frac{(n-1)\Gamma^2(n-3/2)}{\Gamma^2(n-1)}.
		\end{align}
	\end{theorem}
	
	
	
	First note that
	\begin{align*}
	\cov(T_1, T_2) &= E(T_1T_2) - E(T_1)E(T_2) \\
	%& = E(c_0\frac{U_1}{S_1} \cdot c_0\frac{U_2}{S_2}) - E(c_0\frac{U_1}{S_1})E( c_0\frac{U_2}{S_2}) \\
	& = c_0^2 \left[E(U_1U_2)E(\frac{1}{S_1S_2}) - E(\frac{U_1}{S_1})E( \frac{U_2}{S_2})\right]   
	~~~~~~~~~\text{ (by lemma \ref{lemmaIndep})}
	\end{align*}
	where $c_0 = \sqrt{\frac{n}{2}}$ and $\text{Var}(T_1) = \text{Var}(c_0\frac{U_1}{S_1})=
	c_0^2\text{Var}(\frac{U_1}{S_1})$. 
	Note that 
	\begin{align}\label{tttTcorrelation}
	\cor(T_1, T_2) & = \frac{\cor(T_1, T_2) }{\sqrt{\text{Var}(T_1) \text{Var}(T_2) }} =
	\frac{E(U_1U_2)E(\frac{1}{S_1S_2}) - E(\frac{U_1}{S_1})E(
		\frac{U_2}{S_2})}{\sqrt{\text{Var}(\frac{U_1}{S_1})\text{Var}(\frac{U_2}{S_2})}} 
	\end{align}
	We need to calculate $E(U_1U_2)$, $E(\frac{1}{S_1S_2})$, $ E(\frac{U_i}{S_i})$ and
	$\text{Var}(\frac{U_i}{S_i})$ for $i =1, 2$. 
	\begin{enumerate}
		\item Note that $U_i\sim N(\Delta_i, \frac{2\sigma_i^2}{n}),  i=1, 2$. 
		\begin{align}\label{eqq1}
		E(U_1U_2)= \cor(U_1, U_2) + E(U_1)E(U_2) = \rho \frac{2\sigma_1\sigma_2}{n} +
		\Delta_1\Delta_2
		\end{align} 
		\item Since $\frac{(n-1)S_{X_1}^2}{\sigma_1^2}$ and $\frac{(n-1)S_{Y_1}^2}{\sigma_1^2}$ are
		independent  and follow $\chi^2(n-1)$, we have $W_{S_1}=\frac{2(n-1)S_1^2}{\sigma_1^2}\sim
		\chi^2(2n-2)$. It can be shown that 
		\[E(W_{S_1}^k)= \frac{2^k\Gamma(n-1+k)}{\Gamma(n-1)}\] 
		Therefore 
		\[E\left(\frac{1}{S_1}\right) =
		\frac{\sqrt{n-1}\Gamma(n-\frac{3}{2})}{\sigma_1\Gamma(n-1)}=\frac{\sqrt{A}}{\sigma_1}, 
		~~\text{Var}\left(\frac{1}{S_1}\right)=\frac{n-1}{\sigma_1^2}\left[\frac{1}{n-2} -
		\frac{\Gamma^2(n-\frac{3}{2})}{\Gamma^2(n-1)}\right]=\frac{A-B}{\sigma_1^2}\]
		Note that $r_s = \cor(\frac{1}{S_1}, \frac{1}{S_2})$, we have 
		\begin{align}\label{eqq2}
		E(\frac{1}{S_1S_2})  = E(\frac{1}{S_1})E(\frac{1}{S_2}) + r_s
		\sqrt{\text{Var}(\frac{1}{S_1})\text{Var}(\frac{1}{S_2})} = \frac{A}{\sigma_1\sigma_2} + r_s
		\frac{A-B}{\sigma_1\sigma_2}
		\end{align}
		\item $U_i\sim N(\Delta_i, \frac{2\sigma_i^2}{n})$ and $\frac{2(n-1)S_i^2}{\sigma_i^2} \sim
		\chi^2(2n-2)$ and by Lemma \ref{lemmaIndep} they are independent, we have 
		\[\frac{\frac{U_i-\Delta_i}{\sqrt{2\sigma_i^2/n}}}{\frac{2(n-1)S_i^2}{\sigma_i^2}/(2n-2)}  =
		\frac{U_i-\Delta_i}{S_i}\sqrt{\frac{n}{2}}\sim t(2n-2)\]
		It follows from $E\left(\frac{U_i-\Delta_i}{S_i}\sqrt{\frac{n}{2}}\right)=0 $ and
		$\text{Var}\left(\frac{U_i-\Delta_i}{S_i}\sqrt{\frac{n}{2}}\right) = \frac{n-1}{n-2}$ that
		\begin{align}\label{eqq3}
		E\left(\frac{U_i}{S_i}\right) &= \frac{\Delta_i}{\sigma_i}\sqrt{A}
		\end{align}
		\begin{align}\label{eqq4}
		\text{Var}\left(\frac{U_i}{S_i}\right)&=\frac{2}{n}A + \frac{\Delta_i^2}{\sigma_i^2}(A-B)
		\end{align}
	\end{enumerate}
	Finally,  the test statistics correlation (\ref{tTestCorrelation}) is obtained by plugging
	(\ref{eq1}--\ref{eq4}) into (\ref{Tcorrelation}).
	
	Up to now we have obtained an exact expression for $\cor(T_1, T_2)$, which depends not only on
	the sample size $n$, but also on $\Delta/\sigma$, the magnitude of DE. The rest of this section
	discusses asymptotic  property of $\cor(T_1, T_2)$ for large sample size.
	\begin{lemma}\label{llemmaLimit}
		Let $A = \frac{n-1}{n-2}$, $B= \frac{(n-1)\Gamma^2(n-3/2)}{\Gamma^2(n-1)}$, then the following
		results hold:
		\begin{enumerate}
			\item $\lim\limits_{n\rightarrow \infty} A = 1$.
			\item $\lim\limits_{n\rightarrow \infty} B = 1$.
			\item $\lim\limits_{n\rightarrow \infty} C = \frac{1}{4 + 2c + 2c^{-1}}$. 
		\end{enumerate}
	\end{lemma}
	\textbf{Proof}: We will use second order Stirling's formula
	\begin{align}\label{sStirling1}
	n! \approx \sqrt{2\pi n}\left(\frac{n}{e}\right)^n(1 + \frac{1}{12n})
	\end{align}
	By Stirling's formula (\ref{Stirling1}) and  $\Gamma(n + \frac{1}{2}) =
	\frac{(2n)!}{4^nn!}\sqrt{\pi}$ 
	\begin{align*}
	\frac{\Gamma^2(n - \frac{3}{2}) }{\Gamma^2(n-1)} & =\left[\frac{\Gamma(n -2 +
		\frac{1}{2})}{\Gamma(n-1)}\right]^2\\
	&  = \left[\frac{(2n-4)!}{[(n-2)!]^2}\left(\frac{1}{4}\right)^{(n-2)}\right]^2\pi\\
	& \approx \left[\frac{\sqrt{2\pi(2n-4)}\left(\frac{2n-4}{e}\right)^{2n-4}\left(1 +
		\frac{1}{12(2n-4)}\right)}{\left[\sqrt{2\pi(n-2)}\left(\frac{n-2}{e}\right)^{2n-4}\left(1 +
		\frac{1}{12(n-2)}\right)\right]^2}\left(\frac{1}{2}\right)^{2n-4}\right]^2\pi \\
	& = \frac{(n-2)\left[(n-2) + \frac{1}{24}\right]^2}{(n-2 + \frac{1}{12})^4}
	\end{align*}
	therefore $\lim\limits _{n\rightarrow\infty}A = \lim\limits_{n\rightarrow\infty}B = 1$. Next, 
	\begin{align*}
	\lim\limits_{n\rightarrow \infty} n(A-B)& =\lim\limits_{n\rightarrow \infty} n\left[\frac{n-1}{n-2}
	- \frac{(n-2)\left[(n-2) + \frac{1}{24}\right]^2}{(n-2 + \frac{1}{12})^4}\right]\\
	& = \lim\limits_{n\rightarrow \infty}  \frac{\frac{1}{4} n(n-1)(n-2)^3 +
		o\left((n-2)^4\right)}{(n-2)\left[(n-2)^4 +  o\left((n-2)^4\right)\right]}\\
	&= \frac{1}{4}
	\end{align*}
	Application of Lemma \ref{lemmaLimit} to (\ref{tTestCorrelation}) gives the limit of test statistics
	correlation, 
	\begin{align}\label{llimitT}
	\rho(T_1, T_2)=\lim\limits_{n \rightarrow \infty} \cor(T_1, T_2) = \frac{\rho +
		\frac{\Delta_1\Delta_2}{8\sigma_1\sigma_2}r_{\text{s}}}{\sqrt{  \left[ 1 +
			\frac{\Delta_1^2}{8\sigma_1^2}\right]\left[ 1 + \frac{\Delta_2^2}{8\sigma_2^2}\right]}}
	\end{align}
	where $r_{\text{s}}$ is defined in (\ref{invScor}). When $\bm \Delta = \bm 0$ then $\lim\limits_{n
		\rightarrow \infty} \cor(T_1, T_2) = \rho$; but when $\bm \Delta \neq \bm 0$, $\lim\limits_{n
		\rightarrow \infty} \cor(T_1, T_2) \neq \rho$ in general. In the next section, we will discuss
	about it in further detail. 
	
	
	
	if $n_1 + n_2 = 2k$ then we have
	\begin{align*}
	B & = (k-1)\left[\frac{\Gamma(k -2 +
		\frac{1}{2})}{\Gamma(k-1)}\right]^2\\
	&  = (k-1)\left[\frac{(2k-4)!}{[(k-2)!]^2}\left(\frac{1}{4}\right)^{(k-2)}\right]^2\pi\\
	& \approx (k-1)\left[\frac{\sqrt{2\pi(2k-4)}\left(\frac{2k-4}{e}\right)^{2k-4}\left(1 +
		\frac{1}{12(2k-4)}\right)}{\left[\sqrt{2\pi(k-2)}\left(\frac{k-2}{e}\right)^{2k-4}\left(1 +
		\frac{1}{12(k-2)}\right)\right]^2}\left(\frac{1}{2}\right)^{2k-4}\right]^2\pi \\
	& = \frac{(k-1)(k-2)\left[(k-2) + \frac{1}{24}\right]^2}{(k-2 + \frac{1}{12})^4}
	\end{align*}
	
	
	Next, 
	\begin{align*}
	\lim\limits_{n\rightarrow \infty} n(A-B)& =\lim\limits_{n\rightarrow \infty} n\left[\frac{n-1}{n-2}
	- \frac{(n-2)\left[(n-2) + \frac{1}{24}\right]^2}{(n-2 + \frac{1}{12})^4}\right]\\
	& = \lim\limits_{n\rightarrow \infty}  \frac{\frac{1}{4} n(n-1)(n-2)^3 +
		o\left((n-2)^4\right)}{(n-2)\left[(n-2)^4 +  o\left((n-2)^4\right)\right]}\\
	&= \frac{1}{4}
	\end{align*}

